{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import gc\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, random_split, Subset, ConcatDataset\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'exp-base-dir':'exps',\n",
    "    'y_target':'delta_e',\n",
    "    # Number of covariates\n",
    "    'dim_x': None,\n",
    "    #func r\n",
    "    'alpha_for_r':10,\n",
    "    #func attack\n",
    "    'attack_lr': 7e-2,\n",
    "    #func train_theta\n",
    "    'opt_lr':0.01,\n",
    "    'advadv_rate':5,\n",
    "    'grad_layer': 4,\n",
    "    'xa_grad_reduce':-0.01,\n",
    "\n",
    "    #dataset\n",
    "    'batch_size':128,\n",
    "    #SAL\n",
    "    'deltaall':20,\n",
    "    'alpha' :0.5,\n",
    "\n",
    "    'theta_epoch':10,\n",
    "    'attack_epoch':2,\n",
    "\n",
    "    'min_weight':1e8,\n",
    "\n",
    "    #network\n",
    "    'network_lr':0.01,\n",
    "    'num_epochs':1000,\n",
    "\n",
    "    #full training set though network no SAL\n",
    "    'FT_epoch':4,\n",
    "\n",
    "    #partial training starts at and num of batch used\n",
    "    'partial_epoch_start':4,\n",
    "    'partial_batch':5,\n",
    "    'partial_epoch_every':5,\n",
    "    'partial_epoch_save':50,\n",
    "\n",
    "\n",
    "    'early_stop':500,\n",
    "\n",
    "    #for adv data store\n",
    "    'current_epoch':0,\n",
    "    'current_batch':0,\n",
    "    'current_theta_epoch':0,\n",
    "    'current_attack_epoch':0,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def create_folder_based_on_time(base_path: os.PathLike):\n",
    "    base_path = Path(base_path)\n",
    "    # Get the current time\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    # Format the time as a string with only date, hour, and minute (e.g., \"2024-01-23_12-30\")\n",
    "    time_str = current_time.strftime(\"%m-%d-%H-%M\")\n",
    "\n",
    "    # Create a folder name based on the formatted time\n",
    "    folder_name = base_path / f\"{config_dict['y_target']}_{time_str}\"\n",
    "\n",
    "\n",
    "    for subfolder in ('adv', 'models', 'sorted'):\n",
    "        os.makedirs(folder_name / subfolder, exist_ok=True)\n",
    "\n",
    "    return folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/nicholas/programs/ood-materials/exps/delta_e_02-03-22-34')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_folder = create_folder_based_on_time(Path.cwd() / config_dict['exp-base-dir'])\n",
    "config_dict['exp-dir'] = created_folder\n",
    "created_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the CSV file\n",
    "# inputdataset =pd.read_feather('./nom_dataset_pred.feather')\n",
    "# #excluded = [\"comp\",'delta_e', 'bandgap']\n",
    "# feature_used_columns = inputdataset.columns[:inputdataset.columns.get_loc('TSNE_x')]\n",
    "# lables = inputdataset.columns[inputdataset.columns.get_loc('Xshift_tsne'):]\n",
    "\n",
    "# config_dict['dim_x']=len(inputdataset.columns[1:inputdataset.columns.get_loc('bandgap')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xshift_tsne     =inputdataset.loc[inputdataset['Xshift_tsne'    ]==1][feature_used_columns]\n",
    "# Xshift_umap     =inputdataset.loc[inputdataset['Xshift_umap'    ]==1][feature_used_columns]\n",
    "# statY_delta_e   =inputdataset.loc[inputdataset['statY_delta_e'  ]==1][feature_used_columns]\n",
    "# infoY_delta_e   =inputdataset.loc[inputdataset['infoY_delta_e'  ]==1][feature_used_columns]\n",
    "# statY_bandgap   =inputdataset.loc[inputdataset['statY_bandgap'  ]==1][feature_used_columns]\n",
    "# infoY_bandgap   =inputdataset.loc[inputdataset['infoY_bandgap'  ]==1][feature_used_columns]\n",
    "# inRand1         =inputdataset.loc[inputdataset['inRand1'        ]==1][feature_used_columns]\n",
    "# inRand2         =inputdataset.loc[inputdataset['inRand2'        ]==1][feature_used_columns]\n",
    "# inRand3         =inputdataset.loc[inputdataset['inRand3'        ]==1][feature_used_columns]\n",
    "# inRand4         =inputdataset.loc[inputdataset['inRand4'        ]==1][feature_used_columns]\n",
    "# inRand5         =inputdataset.loc[inputdataset['inRand5'        ]==1][feature_used_columns]\n",
    "# inPizoe         =inputdataset.loc[inputdataset['inPizoe'        ]==1][feature_used_columns]\n",
    "\n",
    "\n",
    "# training_set = inputdataset[(inputdataset[lables] == 0).all(axis=1)][feature_used_columns]\n",
    "\n",
    "\n",
    "# # Step 3: Define a custom PyTorch Dataset class\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.inputs = df.drop(columns=['delta_e','comp','bandgap']).values\n",
    "#         self.labels = df['delta_e'].values\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.inputs)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         input = self.inputs[index].tolist()[:]\n",
    "#         label = self.labels[index].tolist()\n",
    "#         return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "#     def getSALdata(self):\n",
    "#         input = np.array(self.inputs[:].tolist())\n",
    "#         label = np.array(self.labels[:].tolist())\n",
    "#         return (input, label)\n",
    "\n",
    "# class RecurrentDataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.inputs = df['data'].values\n",
    "#         self.labels = df['label'].values\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.inputs)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         input = self.inputs[index].tolist()[:]\n",
    "#         label = self.labels[index].tolist()\n",
    "#         return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# # Step 4: Use DataLoader to create batches\n",
    "# batch_size = config_dict['batch_size']\n",
    "\n",
    "# train_dataset = MyDataset(training_set)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "# Xshift_tsne_loader      =DataLoader(MyDataset(Xshift_tsne  ),batch_size=len(Xshift_tsne  ))\n",
    "# Xshift_umap_loader      =DataLoader(MyDataset(Xshift_umap  ),batch_size=len(Xshift_umap  ))\n",
    "# statY_delta_e_loader    =DataLoader(MyDataset(statY_delta_e),batch_size=len(statY_delta_e))\n",
    "# infoY_delta_e_loader    =DataLoader(MyDataset(infoY_delta_e),batch_size=len(infoY_delta_e))\n",
    "# statY_bandgap_loader    =DataLoader(MyDataset(statY_bandgap),batch_size=len(statY_bandgap))\n",
    "# infoY_bandgap_loader    =DataLoader(MyDataset(infoY_bandgap),batch_size=len(infoY_bandgap))\n",
    "# Rsplt_testset1_loader   =DataLoader(MyDataset(inRand1      ),batch_size=len(inRand1      ))\n",
    "# Rsplt_testset2_loader   =DataLoader(MyDataset(inRand2      ),batch_size=len(inRand2      ))\n",
    "# Rsplt_testset3_loader   =DataLoader(MyDataset(inRand3      ),batch_size=len(inRand3      ))\n",
    "# Rsplt_testset4_loader   =DataLoader(MyDataset(inRand4      ),batch_size=len(inRand4      ))\n",
    "# Rsplt_testset5_loader   =DataLoader(MyDataset(inRand5      ),batch_size=len(inRand5      ))\n",
    "# piezo_test_loader       =DataLoader(MyDataset(inPizoe      ),batch_size=len(inPizoe      ))\n",
    "\n",
    "\n",
    "\n",
    "# data=train_dataset.getSALdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the CSV file\n",
    "\n",
    "Rsplt_testset =pd.read_csv('./dataset/Rsplt_testset.csv', index_col=None)\n",
    "Xshft_testset =pd.read_csv('./dataset/Xshft_testset.csv', index_col=None)\n",
    "piezo_testset =pd.read_csv('./dataset/piezo_testset.csv', index_col=None)\n",
    "statY_testset =pd.read_csv('./dataset/statY_testset.csv', index_col=None)\n",
    "infoY_testset =pd.read_csv('./dataset/infoY_testset.csv', index_col=None)\n",
    "final_train   =pd.read_csv('./dataset/final_trainset.csv',index_col=None)\n",
    "\n",
    "Rsplt_testset1 =pd.read_csv('./dataset/Rsplt_testset1.csv', index_col=None)\n",
    "Rsplt_testset2 =pd.read_csv('./dataset/Rsplt_testset2.csv', index_col=None)\n",
    "Rsplt_testset3 =pd.read_csv('./dataset/Rsplt_testset3.csv', index_col=None)\n",
    "Rsplt_testset4 =pd.read_csv('./dataset/Rsplt_testset4.csv', index_col=None)\n",
    "Rsplt_testset5 =pd.read_csv('./dataset/Rsplt_testset5.csv', index_col=None)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define a custom PyTorch Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df.drop(columns=['delta_e','pretty_comp']).values\n",
    "        self.labels = df['delta_e'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index].tolist()[:]\n",
    "        label = self.labels[index].tolist()\n",
    "        return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "    def getSALdata(self):\n",
    "        input = np.array(self.inputs[:].tolist())\n",
    "        label = np.array(self.labels[:].tolist())\n",
    "        return (input, label)\n",
    "\n",
    "class RecurrentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['data'].values\n",
    "        self.labels = df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index].tolist()[:]\n",
    "        label = self.labels[index].tolist()\n",
    "        return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Step 4: Use DataLoader to create batches\n",
    "batch_size = config_dict['batch_size']\n",
    "\n",
    "train_dataset = MyDataset(final_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "Rsplt_test_dataset = MyDataset(Rsplt_testset)\n",
    "Xshft_test_dataset = MyDataset(Xshft_testset)\n",
    "piezo_test_dataset = MyDataset(piezo_testset)\n",
    "statY_test_dataset = MyDataset(statY_testset)\n",
    "infoY_test_dataset = MyDataset(infoY_testset)\n",
    "\n",
    "Rsplt_testset1_dataset = MyDataset(Rsplt_testset1)\n",
    "Rsplt_testset2_dataset = MyDataset(Rsplt_testset2)\n",
    "Rsplt_testset3_dataset = MyDataset(Rsplt_testset3)\n",
    "Rsplt_testset4_dataset = MyDataset(Rsplt_testset4)\n",
    "Rsplt_testset5_dataset = MyDataset(Rsplt_testset5)\n",
    "\n",
    "Rsplt_testset1_loader = DataLoader(Rsplt_testset1_dataset, batch_size=len(Rsplt_testset1))\n",
    "Rsplt_testset2_loader = DataLoader(Rsplt_testset2_dataset, batch_size=len(Rsplt_testset2))\n",
    "Rsplt_testset3_loader = DataLoader(Rsplt_testset3_dataset, batch_size=len(Rsplt_testset3))\n",
    "Rsplt_testset4_loader = DataLoader(Rsplt_testset4_dataset, batch_size=len(Rsplt_testset4))\n",
    "Rsplt_testset5_loader = DataLoader(Rsplt_testset5_dataset, batch_size=len(Rsplt_testset5))\n",
    "\n",
    "Rsplt_test_loader = DataLoader(Rsplt_test_dataset, batch_size=len(Rsplt_testset))\n",
    "Xshft_test_loader = DataLoader(Xshft_test_dataset, batch_size=len(Xshft_testset))\n",
    "piezo_test_loader = DataLoader(piezo_test_dataset, batch_size=len(piezo_testset))\n",
    "statY_test_loader = DataLoader(statY_test_dataset, batch_size=len(statY_testset))\n",
    "infoY_test_loader = DataLoader(infoY_test_dataset, batch_size=len(infoY_testset))\n",
    "\n",
    "\n",
    "data=train_dataset.getSALdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_remove_from_testset(test_dataloader, num_samples=config_dict['batch_size']):\n",
    "    # Step 1: Get the entire test set\n",
    "    test_dataset = test_dataloader.dataset\n",
    "    seed=69\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Step 2: Sample specified number of indices from the test set\n",
    "    sampled_indices = torch.randperm(len(test_dataset))\n",
    "\n",
    "    # Set seed again to ensure consistency in the order of indices between runs\n",
    "    torch.manual_seed(seed)\n",
    "    # Select the first num_samples indices from the shuffled indices\n",
    "    sampled_indices = sampled_indices[:num_samples]\n",
    "\n",
    "    # Step 3: Create a DataLoader with the sampled test batch\n",
    "    sampled_test_dataset = Subset(test_dataset, sampled_indices)\n",
    "    sampled_test_dataloader = DataLoader(sampled_test_dataset, batch_size=num_samples, shuffle=True)\n",
    "\n",
    "    # Step 4: Create a DataLoader with the remaining test set (excluding the sampled indices)\n",
    "    remaining_indices = [i for i in range(len(test_dataset)) if i not in sampled_indices]\n",
    "    remaining_test_dataset = Subset(test_dataset, remaining_indices)\n",
    "    remaining_test_dataloader = DataLoader(remaining_test_dataset, batch_size=len(remaining_test_dataset), shuffle=True)\n",
    "\n",
    "    # Step 5: Print the lengths of the original test set, sampled test set, and remaining test set\n",
    "    print(f\"Original Test Set Size: {len(test_dataset)}\")\n",
    "    print(f\"Sampled Test Set Size: {len(sampled_test_dataloader.dataset)}\")\n",
    "    print(f\"Remaining Test Set Size: {len(remaining_test_dataloader.dataset)}\")\n",
    "\n",
    "    return sampled_test_dataloader, remaining_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test Set Size: 382\n",
      "Sampled Test Set Size: 128\n",
      "Remaining Test Set Size: 254\n"
     ]
    }
   ],
   "source": [
    "piezo_adv_loader,piezo1_test_loader = sample_and_remove_from_testset(piezo_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feat_col_name.txt', 'r') as file:\n",
    "   column_names = file.read().split()\n",
    "# column_names = feature_used_columns\n",
    "def save_tensor(ori_data,ori_lab,adv_data,adv_lab,epoch,batch_idx):\n",
    "    ori_data_list  = ori_data.clone().cpu().numpy()\n",
    "    ori_labels_list = ori_lab.clone().cpu().numpy()\n",
    "    adv_data_list  = adv_data.clone().cpu().numpy()\n",
    "    adv_labels_list = adv_lab.clone().cpu().tolist()\n",
    "\n",
    "\n",
    "    csv_filename = config_dict[\"exp-dir\"] / 'adv' / f'adv_data_labels_in_attack{epoch}_{batch_idx}.csv'\n",
    "\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write header\n",
    "        header = column_names+ ['label']#[f'feature_{i}' for i in range(ori_data_list.shape[1])]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write data and labels for each group\n",
    "        rows_group1 = np.column_stack((ori_data_list, ori_labels_list))\n",
    "        rows_group2 = np.column_stack((adv_data_list, ori_labels_list))\n",
    "\n",
    "\n",
    "        # Combine data from the first two groups only\n",
    "        all_rows = np.vstack((rows_group1, rows_group2))\n",
    "\n",
    "        writer.writerows(all_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRNet_intorch(torch.nn.Module):\n",
    "    #'128-64-16'\n",
    "    def __init__(self, input_size):\n",
    "        super(IRNet_intorch, self).__init__()\n",
    "        self.fc128 =nn.Linear(128, 128)\n",
    "        self.fc64 =nn.Linear(64, 64)\n",
    "        self.fc16 =nn.Linear(16, 16)\n",
    "\n",
    "        self.bn128 =nn.BatchNorm1d(128)\n",
    "        self.bn64 =nn.BatchNorm1d(64)\n",
    "        self.bn16 =nn.BatchNorm1d(16)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.inputlayer = nn.Linear(input_size, 128)\n",
    "\n",
    "        self.con128_64 = nn.Linear(128, 64)\n",
    "        self.con64_16 = nn.Linear(64,16)\n",
    "        self.output16 = nn.Linear(16,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inputlayer(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc128(x)\n",
    "        x = self.bn128(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "        x = self.con128_64(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc64(x)\n",
    "        x = self.bn64(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "        x = self.con64_16(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc16(x)\n",
    "        x = self.bn16(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "\n",
    "        x = self.output16(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best_loss      = float('inf')\n",
    "partial_train_best_loss= float('inf')\n",
    "\n",
    "Rsplt_ave_best_loss = float('inf')\n",
    "\n",
    "#Xshift_tsne_best_loss = float('inf')\n",
    "#Xshift_umap_best_loss = float('inf')\n",
    "#statY_delta_e_best_loss = float('inf')\n",
    "#infoY_delta_e_best_loss = float('inf')\n",
    "#statY_bandgap_best_loss = float('inf')\n",
    "#infoY_bandgap_best_loss = float('inf')\n",
    "#Rsplt_testset1_best_loss = float('inf')\n",
    "#Rsplt_testset2_best_loss = float('inf')\n",
    "#Rsplt_testset3_best_loss = float('inf')\n",
    "#Rsplt_testset4_best_loss = float('inf')\n",
    "#Rsplt_testset5_best_loss = float('inf')\n",
    "#piezo_test_best_loss = float('inf')\n",
    "\n",
    "loss_df = pd.DataFrame(columns=[\n",
    "                        'epoch',\n",
    "                        'train',\n",
    "                        'partialtrain',\n",
    "                        'Xshift_tsne'  ,\n",
    "                        'Xshift_umap'  ,\n",
    "                        'statY_delta_e',\n",
    "                        'infoY_delta_e',\n",
    "                        'statY_bandgap',\n",
    "                        'infoY_bandgap',\n",
    "                        'inRand1'      ,\n",
    "                        'inRand2'      ,\n",
    "                        'inRand3'      ,\n",
    "                        'inRand4'      ,\n",
    "                        'inRand5'      ,\n",
    "                        'inPizoe'      ,\n",
    "                        'RspltAVE',\n",
    "                        'save',\n",
    "                        'attack_gamma'\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(vector):\n",
    "    if type(vector) is list:\n",
    "        vlist = vector\n",
    "    elif type(vector) is np.ndarray:\n",
    "        vlist = vector.reshape(-1).tolist()\n",
    "    else:\n",
    "        vlist = vector.view(-1).tolist()\n",
    "\n",
    "    return \"[\" + \", \".join(\"{:+.4f}\".format(vi) for vi in vlist) + \"]\"\n",
    "\n",
    "# Optimizer Class to maximize loss of adversarial dataset\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=1e-3, beta1=0.9, beta2=0.9, epsilon=1e-8):\n",
    "        self.device = torch.device(device)\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.m_hat = None\n",
    "        self.v_hat = None\n",
    "        self.initialize = False\n",
    "\n",
    "    # Grad = adv_gradient\n",
    "    # Iternum = iteration\n",
    "    # theta = adv_images\n",
    "    # Gradient ascent\n",
    "    def update(self, grad, iternum, theta):\n",
    "        if not self.initialize:\n",
    "            self.m = (1 - self.beta1) * grad\n",
    "            self.v = (1 - self.beta2) * grad ** 2\n",
    "            self.initialize = True\n",
    "        else:\n",
    "            assert self.m.shape == grad.shape\n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * grad ** 2\n",
    "\n",
    "        self.m_hat = self.m / (1 - self.beta1 ** iternum)\n",
    "        self.v_hat = self.v / (1 - self.beta2 ** iternum)\n",
    "        return theta + self.lr * self.m_hat / (self.epsilon + torch.sqrt(self.v_hat))\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "def save_config(config, file_path):\n",
    "    with Path(file_path).open('w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)\n",
    "\n",
    "# Load the configuration from the YAML file\n",
    "def load_config(file_path):\n",
    "    with Path(file_path).open('r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConstructorError",
     "evalue": "could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:pathlib.PosixPath'\n  in \"/home/nicholas/programs/ood-materials/exps/delta_e_02-03-22-34/config.yaml\", line 16, column 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConstructorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m save_config(config_dict, config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp-dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m loaded_config \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexp-dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded Configuration:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loaded_config)\n",
      "Cell \u001b[0;32mIn[13], line 51\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_config\u001b[39m(file_path):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Path(file_path)\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 51\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_load\u001b[39m(stream):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m loader \u001b[38;5;241m=\u001b[39m Loader(stream)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     loader\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/constructor.py:51\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_single_node()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/constructor.py:60\u001b[0m, in \u001b[0;36mBaseConstructor.construct_document\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_generators \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m generator \u001b[38;5;129;01min\u001b[39;00m state_generators:\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dummy \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstructed_objects \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/constructor.py:413\u001b[0m, in \u001b[0;36mSafeConstructor.construct_yaml_map\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    411\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m data\n\u001b[0;32m--> 413\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m data\u001b[38;5;241m.\u001b[39mupdate(value)\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/constructor.py:218\u001b[0m, in \u001b[0;36mSafeConstructor.construct_mapping\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, MappingNode):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_mapping(node)\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/constructor.py:143\u001b[0m, in \u001b[0;36mBaseConstructor.construct_mapping\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mHashable):\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile constructing a mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m, node\u001b[38;5;241m.\u001b[39mstart_mark,\n\u001b[1;32m    142\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound unhashable key\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_node\u001b[38;5;241m.\u001b[39mstart_mark)\n\u001b[0;32m--> 143\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     mapping[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/constructor.py:100\u001b[0m, in \u001b[0;36mBaseConstructor.construct_object\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m     98\u001b[0m             constructor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_mapping\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag_suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     data \u001b[38;5;241m=\u001b[39m constructor(\u001b[38;5;28mself\u001b[39m, tag_suffix, node)\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/yaml/constructor.py:427\u001b[0m, in \u001b[0;36mSafeConstructor.construct_undefined\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_undefined\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    428\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not determine a constructor for the tag \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m node\u001b[38;5;241m.\u001b[39mtag,\n\u001b[1;32m    429\u001b[0m             node\u001b[38;5;241m.\u001b[39mstart_mark)\n",
      "\u001b[0;31mConstructorError\u001b[0m: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:pathlib.PosixPath'\n  in \"/home/nicholas/programs/ood-materials/exps/delta_e_02-03-22-34/config.yaml\", line 16, column 10"
     ]
    }
   ],
   "source": [
    "save_config(config_dict, config_dict[\"exp-dir\"] / 'config.yaml')\n",
    "loaded_config = load_config(config_dict[\"exp-dir\"] / 'config.yaml')\n",
    "print(\"Loaded Configuration:\", loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3697, 150)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.getSALdata()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableAL:\n",
    "    def __init__(self, environment):\n",
    "        self.weights = None\n",
    "        self.model = None\n",
    "        self.weight_grad = None\n",
    "        self.xa_grad = None\n",
    "        self.theta_grad = None\n",
    "        self.gamma = None\n",
    "        self.adversarial_data = None\n",
    "        self.loss_criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.adv_based_on = None\n",
    "        self.adv_again = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        # init\n",
    "        # Number of covariates\n",
    "        dim_x = 150\n",
    "\n",
    "        self.model = IRNet_intorch(dim_x).to(device)\n",
    "        # Covariate Weights\n",
    "        self.weights = torch.zeros(dim_x).reshape(-1, 1) + 100.0\n",
    "        self.weights = self.weights.to(device)\n",
    "\n",
    "    def cost_function(self, x, x_adv):\n",
    "        # Variable cost level where the weights determine the cost level\n",
    "        cost = torch.mean(((x - x_adv) ** 2).mm(self.weights)).to(device)\n",
    "        return cost\n",
    "\n",
    "    # Loss across Training environments\n",
    "    # Self.loss_criterion = MSELoss\n",
    "    def r(self, environments, alpha=config_dict['alpha_for_r']):\n",
    "        result = 0.0\n",
    "        env_loss = []\n",
    "        for x_e, y_e in environments:\n",
    "            x_e = x_e.to(device)\n",
    "            y_e = y_e.to(torch.float32).to(device)\n",
    "            env_loss.append(self.loss_criterion(self.model(x_e), y_e))\n",
    "        env_loss = torch.Tensor(env_loss)\n",
    "        max_index = torch.argmax(env_loss)\n",
    "        min_index = torch.argmin(env_loss)\n",
    "\n",
    "        for idx, (x_e, y_e) in enumerate(environments):\n",
    "            x_e = x_e.to(device)\n",
    "            y_e = y_e.to(torch.float32).to(device)\n",
    "            if idx == max_index:\n",
    "                result += (alpha + 1) * self.loss_criterion(self.model(x_e), y_e)\n",
    "            elif idx == min_index:\n",
    "                result += (1 - alpha) * self.loss_criterion(self.model(x_e), y_e)\n",
    "            else:\n",
    "                result += self.loss_criterion(self.model(x_e), y_e)\n",
    "        return result\n",
    "\n",
    "    # generate adversarial data\n",
    "    # Maximize the loss using their own ADAM.update method(their own optimizer)\n",
    "    def attack(self, gamma, data, step, epoch, batch_idx):\n",
    "        attack_lr = config_dict['attack_lr']\n",
    "        images, labels = data\n",
    "        images_adv = images.clone().detach()\n",
    "\n",
    "        optimizer = Adam(learning_rate=attack_lr)\n",
    "\n",
    "        for i in range(step):\n",
    "            if images_adv.grad is not None:\n",
    "                images_adv.grad.data.zero_()\n",
    "\n",
    "            images_adv = images_adv.to(device)\n",
    "            images_adv.requires_grad_(True)\n",
    "            outputs = self.model(images_adv)\n",
    "\n",
    "            labels = labels.float().to(device)\n",
    "            images = images.to(device)\n",
    "            loss = self.loss_criterion(outputs, labels) - gamma * self.cost_function(\n",
    "                images, images_adv\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            images_adv.data = optimizer.update(images_adv.grad, i + 1, images_adv)\n",
    "\n",
    "        self.weight_grad = -2 * gamma * attack_lr * (images_adv - images)\n",
    "        temp_image = images_adv.clone().detach()\n",
    "        temp_label = labels.clone().detach()\n",
    "        self.adversarial_data = (temp_image, temp_label)\n",
    "\n",
    "        # save adv and ori data\n",
    "        save_tensor(images, labels, temp_image, temp_label, epoch, batch_idx)\n",
    "\n",
    "        return images_adv, labels\n",
    "\n",
    "    # Optimizes the model paremeters such that the loss is minimized\n",
    "    # on the adversarial data from self.attack\n",
    "    def train_theta(\n",
    "        self, data, epochs_theta, epoch_attack, gamma, end_flag=False, epoch=0, batch_idx=None\n",
    "    ):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=config_dict['opt_lr'])\n",
    "        self.adv_based_on = data\n",
    "        # For __ Theta epochs_theta\n",
    "        for i_theta in range(epochs_theta):\n",
    "            if i_theta % config_dict['advadv_rate'] == 0 or not end_flag:\n",
    "                images_adv, labels = self.attack(\n",
    "                    gamma, data, step=epoch_attack, epoch=epoch, batch_idx=batch_idx\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                self.adv_again = self.adversarial_data\n",
    "                images_adv, labels = self.attack(\n",
    "                    gamma,\n",
    "                    self.adversarial_data,\n",
    "                    step=epoch_attack,\n",
    "                    epoch=epoch,\n",
    "                    batch_idx=batch_idx,\n",
    "                )\n",
    "\n",
    "            # print(f\"original data: {data[0].shape}\")\n",
    "            # print(f\"attack data: {images_adv.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            images_adv = images_adv.to(device)\n",
    "            outputs = self.model(images_adv)\n",
    "            loss = self.loss_criterion(outputs, labels.float())\n",
    "\n",
    "            if self.xa_grad is None:\n",
    "                dtheta_dx = []\n",
    "                dloss_dtheta = grad(loss, self.model.parameters(), create_graph=True)[\n",
    "                    config_dict['grad_layer']\n",
    "                ].reshape(-1)\n",
    "\n",
    "                # size dloss = model.para size\n",
    "                for name1, param in self.model.named_parameters():\n",
    "                    print(f'grad: {name1}          {param.shape}')\n",
    "\n",
    "                # time.sleep(5.5)    # Pause 5.5 seconds\n",
    "                print(f'dloss_dtheta.shape:  {dloss_dtheta.shape[0]}')\n",
    "                for j in range(dloss_dtheta.shape[0]):\n",
    "                    # print(f\"dloss_dtheta.shape[0]:j     {j}\")\n",
    "                    dtheta_dx.append(\n",
    "                        grad(dloss_dtheta[j], images_adv, create_graph=True)[0].detach()\n",
    "                    )\n",
    "\n",
    "                self.xa_grad = torch.stack(dtheta_dx, 1).detach()\n",
    "\n",
    "            else:\n",
    "                dloss_dtheta = grad(loss, self.model.parameters(), create_graph=True)[\n",
    "                    config_dict['grad_layer']\n",
    "                ].reshape(-1)\n",
    "                dtheta_dx = []\n",
    "\n",
    "                for j in range(dloss_dtheta.shape[0]):\n",
    "                    dtheta_dx.append(\n",
    "                        grad(dloss_dtheta[j], images_adv, create_graph=True)[0].detach()\n",
    "                    )\n",
    "                self.xa_grad += torch.stack(dtheta_dx, 1).detach()\n",
    "\n",
    "            # print(f\"xa_grad size: {self.xa_grad.shape}\")\n",
    "            del dtheta_dx\n",
    "            del dloss_dtheta\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # print('%d | %.4f | %s'%(i, loss, pretty(self.model.layer[4].weight)))\n",
    "            # if i % 1000 == 999:\n",
    "\n",
    "            # print(f\"loss?\")\n",
    "            loss.backward(retain_graph=True)\n",
    "            # print(f\"step?\")\n",
    "            optimizer.step()\n",
    "            # print(f\"step!\")\n",
    "        self.xa_grad *= config_dict['xa_grad_reduce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([128, 150])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n",
      "torch.Size([16, 64])\n",
      "torch.Size([16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param in method.model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = StableAL([train_dataset.getSALdata()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data,adv_target= next(iter(piezo_adv_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current in epoch    0      batch 0\n",
      "current in epoch    0      batch 1\n",
      "current in epoch    0      batch 2\n",
      "current in epoch    0      batch 3\n",
      "current in epoch    0      batch 4\n",
      "current in epoch    0      batch 5\n",
      "current in epoch    0      batch 6\n",
      "current in epoch    0      batch 7\n",
      "current in epoch    0      batch 8\n",
      "current in epoch    0      batch 9\n",
      "current in epoch    0      batch 10\n",
      "current in epoch    0      batch 11\n",
      "current in epoch    0      batch 12\n",
      "current in epoch    0      batch 13\n",
      "current in epoch    0      batch 14\n",
      "current in epoch    0      batch 15\n",
      "current in epoch    0      batch 16\n",
      "current in epoch    0      batch 17\n",
      "current in epoch    0      batch 18\n",
      "current in epoch    0      batch 19\n",
      "current in epoch    0      batch 20\n",
      "current in epoch    0      batch 21\n",
      "current in epoch    0      batch 22\n",
      "current in epoch    0      batch 23\n",
      "current in epoch    0      batch 24\n",
      "current in epoch    0      batch 25\n",
      "current in epoch    0      batch 26\n",
      "current in epoch    0      batch 27\n",
      "========================================\n",
      "Epoch 1/1000 - partial_train_loss: 30023.2189 \n",
      "sorting training set\n",
      "Epoch 1/1000 - Training loss: 1656.7981 \n",
      "========================================\n",
      "Epoch: [1/1000], TrainLoss: 1756.788912984106\n",
      "training Loss has not improved for 2 epochs.\n",
      "current in epoch    1      batch 0\n",
      "current in epoch    1      batch 1\n",
      "current in epoch    1      batch 2\n",
      "current in epoch    1      batch 3\n",
      "current in epoch    1      batch 4\n",
      "current in epoch    1      batch 5\n",
      "current in epoch    1      batch 6\n",
      "current in epoch    1      batch 7\n",
      "current in epoch    1      batch 8\n",
      "current in epoch    1      batch 9\n",
      "current in epoch    1      batch 10\n",
      "current in epoch    1      batch 11\n",
      "current in epoch    1      batch 12\n",
      "current in epoch    1      batch 13\n",
      "current in epoch    1      batch 14\n",
      "current in epoch    1      batch 15\n",
      "current in epoch    1      batch 16\n",
      "current in epoch    1      batch 17\n",
      "current in epoch    1      batch 18\n",
      "current in epoch    1      batch 19\n",
      "current in epoch    1      batch 20\n",
      "current in epoch    1      batch 21\n",
      "current in epoch    1      batch 22\n",
      "current in epoch    1      batch 23\n",
      "current in epoch    1      batch 24\n",
      "current in epoch    1      batch 25\n",
      "current in epoch    1      batch 26\n",
      "current in epoch    1      batch 27\n",
      "========================================\n",
      "Epoch 2/1000 - partial_train_loss: 904.4956 \n",
      "Epoch: [2/1000], TrainLoss: 131.2986891610282\n",
      "training Loss has not improved for 3 epochs.\n",
      "current in epoch    2      batch 0\n",
      "current in epoch    2      batch 1\n",
      "current in epoch    2      batch 2\n",
      "current in epoch    2      batch 3\n",
      "current in epoch    2      batch 4\n",
      "current in epoch    2      batch 5\n",
      "current in epoch    2      batch 6\n",
      "current in epoch    2      batch 7\n",
      "current in epoch    2      batch 8\n",
      "current in epoch    2      batch 9\n",
      "current in epoch    2      batch 10\n",
      "current in epoch    2      batch 11\n",
      "current in epoch    2      batch 12\n",
      "current in epoch    2      batch 13\n",
      "current in epoch    2      batch 14\n",
      "current in epoch    2      batch 15\n",
      "current in epoch    2      batch 16\n",
      "current in epoch    2      batch 17\n",
      "current in epoch    2      batch 18\n",
      "current in epoch    2      batch 19\n",
      "current in epoch    2      batch 20\n",
      "current in epoch    2      batch 21\n",
      "current in epoch    2      batch 22\n",
      "current in epoch    2      batch 23\n",
      "current in epoch    2      batch 24\n",
      "current in epoch    2      batch 25\n",
      "current in epoch    2      batch 26\n",
      "current in epoch    2      batch 27\n",
      "========================================\n",
      "Epoch 3/1000 - partial_train_loss: 34.1184 \n",
      "Epoch: [3/1000], TrainLoss: 41.42762793813433\n",
      "training Loss has not improved for 4 epochs.\n",
      "current in epoch    3      batch 0\n",
      "current in epoch    3      batch 1\n",
      "current in epoch    3      batch 2\n",
      "current in epoch    3      batch 3\n",
      "current in epoch    3      batch 4\n",
      "current in epoch    3      batch 5\n",
      "current in epoch    3      batch 6\n",
      "current in epoch    3      batch 7\n",
      "current in epoch    3      batch 8\n",
      "current in epoch    3      batch 9\n",
      "current in epoch    3      batch 10\n",
      "current in epoch    3      batch 11\n",
      "current in epoch    3      batch 12\n",
      "current in epoch    3      batch 13\n",
      "current in epoch    3      batch 14\n",
      "current in epoch    3      batch 15\n",
      "current in epoch    3      batch 16\n",
      "current in epoch    3      batch 17\n",
      "current in epoch    3      batch 18\n",
      "current in epoch    3      batch 19\n",
      "current in epoch    3      batch 20\n",
      "current in epoch    3      batch 21\n",
      "current in epoch    3      batch 22\n",
      "current in epoch    3      batch 23\n",
      "current in epoch    3      batch 24\n",
      "current in epoch    3      batch 25\n",
      "current in epoch    3      batch 26\n",
      "current in epoch    3      batch 27\n",
      "========================================\n",
      "Epoch 4/1000 - partial_train_loss: 27.4593 \n",
      "Epoch: [4/1000], TrainLoss: 22.45572902900832\n",
      "training Loss has not improved for 5 epochs.\n",
      "current in epoch    4      batch 0\n",
      "RLoss: 80.43958282470703\n",
      "current in epoch    4      batch 1\n",
      "RLoss: 792.287353515625\n",
      "current in epoch    4      batch 2\n",
      "RLoss: 549.7440795898438\n",
      "current in epoch    4      batch 3\n",
      "RLoss: 110.71696472167969\n",
      "current in epoch    4      batch 4\n",
      "RLoss: 1151.753662109375\n",
      "current in epoch    4      batch 5\n",
      "RLoss: 1735.603271484375\n",
      "current in epoch    4      batch 6\n",
      "RLoss: 2341.691650390625\n",
      "current in epoch    4      batch 7\n",
      "RLoss: 4586.3271484375\n",
      "current in epoch    4      batch 8\n",
      "RLoss: 224.14727783203125\n",
      "current in epoch    4      batch 9\n",
      "RLoss: 410.31024169921875\n",
      "current in epoch    4      batch 10\n",
      "RLoss: 254.35618591308594\n",
      "current in epoch    4      batch 11\n",
      "RLoss: 422.7749328613281\n",
      "current in epoch    4      batch 12\n",
      "RLoss: 850.9420776367188\n",
      "current in epoch    4      batch 13\n",
      "RLoss: 26.728307723999023\n",
      "current in epoch    4      batch 14\n",
      "RLoss: 717.672607421875\n",
      "current in epoch    4      batch 15\n",
      "RLoss: 232.68338012695312\n",
      "current in epoch    4      batch 16\n",
      "RLoss: 4.4166178703308105\n",
      "current in epoch    4      batch 17\n",
      "RLoss: 11.609127044677734\n",
      "current in epoch    4      batch 18\n",
      "RLoss: 210.115966796875\n",
      "current in epoch    4      batch 19\n",
      "RLoss: 375.2213439941406\n",
      "current in epoch    4      batch 20\n",
      "RLoss: 1102.266357421875\n",
      "current in epoch    4      batch 21\n",
      "RLoss: 169.62611389160156\n",
      "current in epoch    4      batch 22\n",
      "RLoss: 989.3707275390625\n",
      "current in epoch    4      batch 23\n",
      "RLoss: 2211.85986328125\n",
      "current in epoch    4      batch 24\n",
      "RLoss: 87.48639678955078\n",
      "current in epoch    4      batch 25\n",
      "RLoss: 909.79736328125\n",
      "current in epoch    4      batch 26\n",
      "RLoss: 421.111328125\n",
      "current in epoch    4      batch 27\n",
      "RLoss: 379.9647216796875\n",
      "========================================\n",
      "Epoch 5/1000 - partial_train_loss: 827.3703 \n",
      "Epoch: [5/1000], TrainLoss: 555.7614299229214\n",
      "training Loss has not improved for 6 epochs.\n",
      "current in epoch    5      batch 0\n",
      "RLoss: 12.45899772644043\n",
      "current in epoch    5      batch 1\n",
      "RLoss: 3023.834228515625\n",
      "current in epoch    5      batch 2\n",
      "RLoss: 1364.9495849609375\n",
      "current in epoch    5      batch 3\n",
      "RLoss: 3.936065912246704\n",
      "current in epoch    5      batch 4\n",
      "RLoss: 143.17855834960938\n",
      "current in epoch    5      batch 5\n",
      "RLoss: 225.79354858398438\n",
      "========================================\n",
      "Epoch 6/1000 - partial_train_loss: 1755.4790 \n",
      "sorting training set\n",
      "Epoch 6/1000 - Training loss: 252.7822 \n",
      "========================================\n",
      "Epoch: [6/1000], TrainLoss: 267.4892122436424\n",
      "training Loss has not improved for 7 epochs.\n",
      "current in epoch    6      batch 0\n",
      "RLoss: 61.57548522949219\n",
      "current in epoch    6      batch 1\n",
      "RLoss: 198.4484405517578\n",
      "current in epoch    6      batch 2\n",
      "RLoss: 376.4877014160156\n",
      "current in epoch    6      batch 3\n",
      "RLoss: 47.95801544189453\n",
      "current in epoch    6      batch 4\n",
      "RLoss: 660.002197265625\n",
      "current in epoch    6      batch 5\n",
      "RLoss: 161.7701873779297\n",
      "========================================\n",
      "Epoch 7/1000 - partial_train_loss: 482.6937 \n",
      "Epoch: [7/1000], TrainLoss: 136.29096739632743\n",
      "training Loss has not improved for 8 epochs.\n",
      "current in epoch    7      batch 0\n",
      "RLoss: 9.078332901000977\n",
      "current in epoch    7      batch 1\n",
      "RLoss: 862.6017456054688\n",
      "current in epoch    7      batch 2\n",
      "RLoss: 307.2223205566406\n",
      "current in epoch    7      batch 3\n",
      "RLoss: 15.61716365814209\n",
      "current in epoch    7      batch 4\n",
      "RLoss: 152.9849853515625\n",
      "current in epoch    7      batch 5\n",
      "RLoss: 548.4160766601562\n",
      "========================================\n",
      "Epoch 8/1000 - partial_train_loss: 584.6951 \n",
      "Epoch: [8/1000], TrainLoss: 616.2739922659738\n",
      "training Loss has not improved for 9 epochs.\n",
      "current in epoch    8      batch 0\n",
      "RLoss: 560.0913696289062\n",
      "current in epoch    8      batch 1\n",
      "RLoss: 3.7582473754882812\n",
      "current in epoch    8      batch 2\n",
      "RLoss: 311.10107421875\n",
      "current in epoch    8      batch 3\n",
      "RLoss: 236.20223999023438\n",
      "current in epoch    8      batch 4\n",
      "RLoss: 1041.966064453125\n",
      "current in epoch    8      batch 5\n",
      "RLoss: 27.869441986083984\n",
      "========================================\n",
      "Epoch 9/1000 - partial_train_loss: 1041.7878 \n",
      "Epoch: [9/1000], TrainLoss: 8.463877490588597\n",
      "training Loss has not improved for 10 epochs.\n",
      "current in epoch    9      batch 0\n",
      "RLoss: 1192.3724365234375\n",
      "current in epoch    9      batch 1\n",
      "RLoss: 1862.485595703125\n",
      "current in epoch    9      batch 2\n",
      "RLoss: 1356.169677734375\n",
      "current in epoch    9      batch 3\n",
      "RLoss: 17.668331146240234\n",
      "current in epoch    9      batch 4\n",
      "RLoss: 34.930477142333984\n",
      "current in epoch    9      batch 5\n",
      "RLoss: 133.82249450683594\n",
      "========================================\n",
      "Epoch 10/1000 - partial_train_loss: 1445.6917 \n",
      "Epoch: [10/1000], TrainLoss: 273.0097231183733\n",
      "training Loss has not improved for 11 epochs.\n",
      "current in epoch    10      batch 0\n",
      "RLoss: 221.26307678222656\n",
      "current in epoch    10      batch 1\n",
      "RLoss: 125.11222839355469\n",
      "current in epoch    10      batch 2\n",
      "RLoss: 1298.9122314453125\n",
      "current in epoch    10      batch 3\n",
      "RLoss: 3016.085693359375\n",
      "current in epoch    10      batch 4\n",
      "RLoss: 260.7741394042969\n",
      "current in epoch    10      batch 5\n",
      "RLoss: 149.3432159423828\n",
      "========================================\n",
      "Epoch 11/1000 - partial_train_loss: 1743.2352 \n",
      "sorting training set\n",
      "Epoch 11/1000 - Training loss: 187.3856 \n",
      "========================================\n",
      "Epoch: [11/1000], TrainLoss: 198.45261424339287\n",
      "training Loss has not improved for 12 epochs.\n",
      "current in epoch    11      batch 0\n",
      "RLoss: 17.98948860168457\n",
      "current in epoch    11      batch 1\n",
      "RLoss: 446.8941650390625\n",
      "current in epoch    11      batch 2\n",
      "RLoss: 148.0215301513672\n",
      "current in epoch    11      batch 3\n",
      "RLoss: 26.96530532836914\n",
      "current in epoch    11      batch 4\n",
      "RLoss: 1726.7138671875\n",
      "current in epoch    11      batch 5\n",
      "RLoss: 1374.90478515625\n",
      "========================================\n",
      "Epoch 12/1000 - partial_train_loss: 652.8695 \n",
      "Epoch: [12/1000], TrainLoss: 1604.9741886683873\n",
      "training Loss has not improved for 13 epochs.\n",
      "current in epoch    12      batch 0\n",
      "RLoss: 441.7903747558594\n",
      "current in epoch    12      batch 1\n",
      "RLoss: 490.8365173339844\n",
      "current in epoch    12      batch 2\n",
      "RLoss: 1286.6014404296875\n",
      "current in epoch    12      batch 3\n",
      "RLoss: 527.6469116210938\n",
      "current in epoch    12      batch 4\n",
      "RLoss: 17.36438751220703\n",
      "current in epoch    12      batch 5\n",
      "RLoss: 452.4061584472656\n",
      "========================================\n",
      "Epoch 13/1000 - partial_train_loss: 1161.0441 \n",
      "Epoch: [13/1000], TrainLoss: 417.9366144452776\n",
      "training Loss has not improved for 14 epochs.\n",
      "current in epoch    13      batch 0\n",
      "RLoss: 279.2259521484375\n",
      "current in epoch    13      batch 1\n",
      "RLoss: 5.678697109222412\n",
      "current in epoch    13      batch 2\n",
      "RLoss: 364.80712890625\n",
      "current in epoch    13      batch 3\n",
      "RLoss: 370.6474304199219\n",
      "current in epoch    13      batch 4\n",
      "RLoss: 7442.10205078125\n",
      "current in epoch    13      batch 5\n",
      "RLoss: 3259.070068359375\n",
      "========================================\n",
      "Epoch 14/1000 - partial_train_loss: 2013.7334 \n",
      "Epoch: [14/1000], TrainLoss: 3583.292493547712\n",
      "training Loss has not improved for 15 epochs.\n",
      "current in epoch    14      batch 0\n",
      "RLoss: 671.277099609375\n",
      "current in epoch    14      batch 1\n",
      "RLoss: 687.6367797851562\n",
      "current in epoch    14      batch 2\n",
      "RLoss: 531.448486328125\n",
      "current in epoch    14      batch 3\n",
      "RLoss: 2010.788818359375\n",
      "current in epoch    14      batch 4\n",
      "RLoss: 62.26662063598633\n",
      "current in epoch    14      batch 5\n",
      "RLoss: 263.7953796386719\n",
      "========================================\n",
      "Epoch 15/1000 - partial_train_loss: 2564.6948 \n",
      "Epoch: [15/1000], TrainLoss: 474.89504895891463\n",
      "training Loss has not improved for 16 epochs.\n",
      "current in epoch    15      batch 0\n",
      "RLoss: 204.66909790039062\n",
      "current in epoch    15      batch 1\n",
      "RLoss: 1498.075927734375\n",
      "current in epoch    15      batch 2\n",
      "RLoss: 1311.43115234375\n",
      "current in epoch    15      batch 3\n",
      "RLoss: 246.02085876464844\n",
      "current in epoch    15      batch 4\n",
      "RLoss: 150.42440795898438\n",
      "current in epoch    15      batch 5\n",
      "RLoss: 755.5994262695312\n",
      "========================================\n",
      "Epoch 16/1000 - partial_train_loss: 1036.9578 \n",
      "sorting training set\n",
      "Epoch 16/1000 - Training loss: 1000.2559 \n",
      "========================================\n",
      "Epoch: [16/1000], TrainLoss: 1058.2690860022026\n",
      "training Loss has not improved for 17 epochs.\n",
      "current in epoch    16      batch 0\n",
      "RLoss: 2116.003662109375\n",
      "current in epoch    16      batch 1\n",
      "RLoss: 1154.2548828125\n",
      "current in epoch    16      batch 2\n",
      "RLoss: 1207.153564453125\n",
      "current in epoch    16      batch 3\n",
      "RLoss: 2622.5927734375\n",
      "current in epoch    16      batch 4\n",
      "RLoss: 8.911742210388184\n",
      "current in epoch    16      batch 5\n",
      "RLoss: 153.41973876953125\n",
      "========================================\n",
      "Epoch 17/1000 - partial_train_loss: 2997.8843 \n",
      "Epoch: [17/1000], TrainLoss: 79.77503027234759\n",
      "training Loss has not improved for 18 epochs.\n",
      "current in epoch    17      batch 0\n",
      "RLoss: 4757.86474609375\n",
      "current in epoch    17      batch 1\n",
      "RLoss: 935.35498046875\n",
      "current in epoch    17      batch 2\n",
      "RLoss: 1192.0040283203125\n",
      "current in epoch    17      batch 3\n",
      "RLoss: 3186.739013671875\n",
      "current in epoch    17      batch 4\n",
      "RLoss: 1629.359130859375\n",
      "current in epoch    17      batch 5\n",
      "RLoss: 445.8970031738281\n",
      "========================================\n",
      "Epoch 18/1000 - partial_train_loss: 3865.1424 \n",
      "Epoch: [18/1000], TrainLoss: 560.6532429286411\n",
      "training Loss has not improved for 19 epochs.\n",
      "current in epoch    18      batch 0\n",
      "RLoss: 9.643559455871582\n",
      "current in epoch    18      batch 1\n",
      "RLoss: 1561.5533447265625\n",
      "current in epoch    18      batch 2\n",
      "RLoss: 69.99822998046875\n",
      "current in epoch    18      batch 3\n",
      "RLoss: 169.9308319091797\n",
      "current in epoch    18      batch 4\n",
      "RLoss: 1740.712890625\n",
      "current in epoch    18      batch 5\n",
      "RLoss: 2431.930419921875\n",
      "========================================\n",
      "Epoch 19/1000 - partial_train_loss: 1048.7990 \n",
      "Epoch: [19/1000], TrainLoss: 2483.5218505859375\n",
      "training Loss has not improved for 20 epochs.\n",
      "current in epoch    19      batch 0\n",
      "RLoss: 1276.763427734375\n",
      "current in epoch    19      batch 1\n",
      "RLoss: 49.257389068603516\n",
      "current in epoch    19      batch 2\n",
      "RLoss: 213.7356414794922\n",
      "current in epoch    19      batch 3\n",
      "RLoss: 493.77362060546875\n",
      "current in epoch    19      batch 4\n",
      "RLoss: 1449.91650390625\n",
      "current in epoch    19      batch 5\n",
      "RLoss: 2358.11572265625\n",
      "========================================\n",
      "Epoch 20/1000 - partial_train_loss: 2285.8480 \n",
      "Epoch: [20/1000], TrainLoss: 7779.07222202846\n",
      "training Loss has not improved for 21 epochs.\n",
      "current in epoch    20      batch 0\n",
      "RLoss: 7.495790958404541\n",
      "current in epoch    20      batch 1\n",
      "RLoss: 537.0347900390625\n",
      "current in epoch    20      batch 2\n",
      "RLoss: 330.197265625\n",
      "current in epoch    20      batch 3\n",
      "RLoss: 499.4822998046875\n",
      "current in epoch    20      batch 4\n",
      "RLoss: 658.8941650390625\n",
      "current in epoch    20      batch 5\n",
      "RLoss: 3153.289306640625\n",
      "========================================\n",
      "Epoch 21/1000 - partial_train_loss: 3407.4641 \n",
      "sorting training set\n",
      "Epoch 21/1000 - Training loss: 3729.9133 \n",
      "========================================\n",
      "Epoch: [21/1000], TrainLoss: 3941.0776180967655\n",
      "training Loss has not improved for 22 epochs.\n",
      "current in epoch    21      batch 0\n",
      "RLoss: 2078.675537109375\n",
      "current in epoch    21      batch 1\n",
      "RLoss: 716.33642578125\n",
      "current in epoch    21      batch 2\n",
      "RLoss: 103.96241760253906\n",
      "current in epoch    21      batch 3\n",
      "RLoss: 3144.12353515625\n",
      "current in epoch    21      batch 4\n",
      "RLoss: 2752.60986328125\n",
      "current in epoch    21      batch 5\n",
      "RLoss: 1038.8990478515625\n",
      "========================================\n",
      "Epoch 22/1000 - partial_train_loss: 4298.4449 \n",
      "Epoch: [22/1000], TrainLoss: 1961.9561222621373\n",
      "training Loss has not improved for 23 epochs.\n",
      "current in epoch    22      batch 0\n",
      "RLoss: 2595.617431640625\n",
      "current in epoch    22      batch 1\n",
      "RLoss: 1580.99169921875\n",
      "current in epoch    22      batch 2\n",
      "RLoss: 1586.8292236328125\n",
      "current in epoch    22      batch 3\n",
      "RLoss: 598.6749877929688\n",
      "current in epoch    22      batch 4\n",
      "RLoss: 244.42123413085938\n",
      "current in epoch    22      batch 5\n",
      "RLoss: 2768.62109375\n",
      "========================================\n",
      "Epoch 23/1000 - partial_train_loss: 3049.4932 \n",
      "Epoch: [23/1000], TrainLoss: 4703.465641566685\n",
      "training Loss has not improved for 24 epochs.\n",
      "current in epoch    23      batch 0\n",
      "RLoss: 43.13485336303711\n",
      "current in epoch    23      batch 1\n",
      "RLoss: 1043.7620849609375\n",
      "current in epoch    23      batch 2\n",
      "RLoss: 721.5963134765625\n",
      "current in epoch    23      batch 3\n",
      "RLoss: 4714.45166015625\n",
      "current in epoch    23      batch 4\n",
      "RLoss: 5424.6240234375\n",
      "current in epoch    23      batch 5\n",
      "RLoss: 2048.756103515625\n",
      "========================================\n",
      "Epoch 24/1000 - partial_train_loss: 6877.0903 \n",
      "Epoch: [24/1000], TrainLoss: 1577.7532980782646\n",
      "training Loss has not improved for 25 epochs.\n",
      "current in epoch    24      batch 0\n",
      "RLoss: 47.71731948852539\n",
      "current in epoch    24      batch 1\n",
      "RLoss: 1235.053466796875\n",
      "current in epoch    24      batch 2\n",
      "RLoss: 194.63343811035156\n",
      "current in epoch    24      batch 3\n",
      "RLoss: 163.78538513183594\n",
      "current in epoch    24      batch 4\n",
      "RLoss: 448.36029052734375\n",
      "current in epoch    24      batch 5\n",
      "RLoss: 259.42144775390625\n",
      "========================================\n",
      "Epoch 25/1000 - partial_train_loss: 1371.0064 \n",
      "Epoch: [25/1000], TrainLoss: 135.0993655068534\n",
      "training Loss has not improved for 26 epochs.\n",
      "current in epoch    25      batch 0\n",
      "RLoss: 1660.3603515625\n",
      "current in epoch    25      batch 1\n",
      "RLoss: 65.82845306396484\n",
      "current in epoch    25      batch 2\n",
      "RLoss: 37.25904846191406\n",
      "current in epoch    25      batch 3\n",
      "RLoss: 57.6769905090332\n",
      "current in epoch    25      batch 4\n",
      "RLoss: 4442.88720703125\n",
      "current in epoch    25      batch 5\n",
      "RLoss: 1345.15185546875\n",
      "========================================\n",
      "Epoch 26/1000 - partial_train_loss: 909.9119 \n",
      "sorting training set\n",
      "Epoch 26/1000 - Training loss: 1602.5335 \n",
      "========================================\n",
      "Epoch: [26/1000], TrainLoss: 1696.2796593866585\n",
      "training Loss has not improved for 27 epochs.\n",
      "current in epoch    26      batch 0\n",
      "RLoss: 2871.45947265625\n",
      "current in epoch    26      batch 1\n",
      "RLoss: 2536.2353515625\n",
      "current in epoch    26      batch 2\n",
      "RLoss: 2619.19677734375\n",
      "current in epoch    26      batch 3\n",
      "RLoss: 1563.9427490234375\n",
      "current in epoch    26      batch 4\n",
      "RLoss: 322.3177185058594\n",
      "current in epoch    26      batch 5\n",
      "RLoss: 3452.916748046875\n",
      "========================================\n",
      "Epoch 27/1000 - partial_train_loss: 3192.1521 \n",
      "Epoch: [27/1000], TrainLoss: 4522.757664271763\n",
      "training Loss has not improved for 28 epochs.\n",
      "current in epoch    27      batch 0\n",
      "RLoss: 29.981515884399414\n",
      "current in epoch    27      batch 1\n",
      "RLoss: 1636.013427734375\n",
      "current in epoch    27      batch 2\n",
      "RLoss: 58.880516052246094\n",
      "current in epoch    27      batch 3\n",
      "RLoss: 72.25846862792969\n",
      "current in epoch    27      batch 4\n",
      "RLoss: 704.8275756835938\n",
      "current in epoch    27      batch 5\n",
      "RLoss: 40.15261459350586\n",
      "========================================\n",
      "Epoch 28/1000 - partial_train_loss: 2475.3310 \n",
      "Epoch: [28/1000], TrainLoss: 21.0929399899074\n",
      "training Loss has not improved for 29 epochs.\n",
      "current in epoch    28      batch 0\n",
      "RLoss: 205.18402099609375\n",
      "current in epoch    28      batch 1\n",
      "RLoss: 1109.1387939453125\n",
      "current in epoch    28      batch 2\n",
      "RLoss: 599.1757202148438\n",
      "current in epoch    28      batch 3\n",
      "RLoss: 277.9010314941406\n",
      "current in epoch    28      batch 4\n",
      "RLoss: 24.539457321166992\n",
      "current in epoch    28      batch 5\n",
      "RLoss: 91.52977752685547\n",
      "========================================\n",
      "Epoch 29/1000 - partial_train_loss: 717.7007 \n",
      "Epoch: [29/1000], TrainLoss: 19.264948470251902\n",
      "training Loss has not improved for 30 epochs.\n",
      "current in epoch    29      batch 0\n",
      "RLoss: 942.5133666992188\n",
      "current in epoch    29      batch 1\n",
      "RLoss: 400.95977783203125\n",
      "current in epoch    29      batch 2\n",
      "RLoss: 1109.3037109375\n",
      "current in epoch    29      batch 3\n",
      "RLoss: 241.58775329589844\n",
      "current in epoch    29      batch 4\n",
      "RLoss: 546.7982177734375\n",
      "current in epoch    29      batch 5\n",
      "RLoss: 1624.53173828125\n",
      "========================================\n",
      "Epoch 30/1000 - partial_train_loss: 829.2746 \n",
      "Epoch: [30/1000], TrainLoss: 1755.265864780971\n",
      "training Loss has not improved for 31 epochs.\n",
      "current in epoch    30      batch 0\n",
      "RLoss: 133.8915252685547\n",
      "current in epoch    30      batch 1\n",
      "RLoss: 1147.12939453125\n",
      "current in epoch    30      batch 2\n",
      "RLoss: 2790.94384765625\n",
      "current in epoch    30      batch 3\n",
      "RLoss: 455.9132385253906\n",
      "current in epoch    30      batch 4\n",
      "RLoss: 1239.383056640625\n",
      "current in epoch    30      batch 5\n",
      "RLoss: 9.038971900939941\n",
      "========================================\n",
      "Epoch 31/1000 - partial_train_loss: 2769.1958 \n",
      "sorting training set\n",
      "Epoch 31/1000 - Training loss: 32.7268 \n",
      "========================================\n",
      "Epoch: [31/1000], TrainLoss: 35.16148257468854\n",
      "training Loss has not improved for 32 epochs.\n",
      "current in epoch    31      batch 0\n",
      "RLoss: 35.962364196777344\n",
      "current in epoch    31      batch 1\n",
      "RLoss: 2619.6181640625\n",
      "current in epoch    31      batch 2\n",
      "RLoss: 675.0762939453125\n",
      "current in epoch    31      batch 3\n",
      "RLoss: 320.81500244140625\n",
      "current in epoch    31      batch 4\n",
      "RLoss: 171.47564697265625\n",
      "current in epoch    31      batch 5\n",
      "RLoss: 1610.1221923828125\n",
      "========================================\n",
      "Epoch 32/1000 - partial_train_loss: 1673.3460 \n",
      "Epoch: [32/1000], TrainLoss: 2697.5369480678014\n",
      "training Loss has not improved for 33 epochs.\n",
      "current in epoch    32      batch 0\n",
      "RLoss: 3167.127685546875\n",
      "current in epoch    32      batch 1\n",
      "RLoss: 3555.6171875\n",
      "current in epoch    32      batch 2\n",
      "RLoss: 1695.0059814453125\n",
      "current in epoch    32      batch 3\n",
      "RLoss: 2314.055419921875\n",
      "current in epoch    32      batch 4\n",
      "RLoss: 493.29705810546875\n",
      "current in epoch    32      batch 5\n",
      "RLoss: 289.6051025390625\n",
      "========================================\n",
      "Epoch 33/1000 - partial_train_loss: 3507.9398 \n",
      "Epoch: [33/1000], TrainLoss: 976.6506832667759\n",
      "training Loss has not improved for 34 epochs.\n",
      "current in epoch    33      batch 0\n",
      "RLoss: 418.5890808105469\n",
      "current in epoch    33      batch 1\n",
      "RLoss: 1394.7991943359375\n",
      "current in epoch    33      batch 2\n",
      "RLoss: 1435.5455322265625\n",
      "current in epoch    33      batch 3\n",
      "RLoss: 451.1844177246094\n",
      "current in epoch    33      batch 4\n",
      "RLoss: 651.8280639648438\n",
      "current in epoch    33      batch 5\n",
      "RLoss: 799.73046875\n",
      "========================================\n",
      "Epoch 34/1000 - partial_train_loss: 1470.2162 \n",
      "Epoch: [34/1000], TrainLoss: 1270.051025390625\n",
      "training Loss has not improved for 35 epochs.\n",
      "current in epoch    34      batch 0\n",
      "RLoss: 13.489168167114258\n",
      "current in epoch    34      batch 1\n",
      "RLoss: 268.8763732910156\n",
      "current in epoch    34      batch 2\n",
      "RLoss: 2960.36572265625\n",
      "current in epoch    34      batch 3\n",
      "RLoss: 388.1865539550781\n",
      "current in epoch    34      batch 4\n",
      "RLoss: 10.269543647766113\n",
      "current in epoch    34      batch 5\n",
      "RLoss: 535.9035034179688\n",
      "========================================\n",
      "Epoch 35/1000 - partial_train_loss: 1627.7996 \n",
      "Epoch: [35/1000], TrainLoss: 1113.2221930367607\n",
      "training Loss has not improved for 36 epochs.\n",
      "current in epoch    35      batch 0\n",
      "RLoss: 1814.5040283203125\n",
      "current in epoch    35      batch 1\n",
      "RLoss: 311.95526123046875\n",
      "current in epoch    35      batch 2\n",
      "RLoss: 3.8721344470977783\n",
      "current in epoch    35      batch 3\n",
      "RLoss: 60.97116470336914\n",
      "current in epoch    35      batch 4\n",
      "RLoss: 57.418373107910156\n",
      "current in epoch    35      batch 5\n",
      "RLoss: 28.7354793548584\n",
      "========================================\n",
      "Epoch 36/1000 - partial_train_loss: 1067.0089 \n",
      "sorting training set\n",
      "Epoch 36/1000 - Training loss: 79.4349 \n",
      "========================================\n",
      "Epoch: [36/1000], TrainLoss: 84.9799397260336\n",
      "training Loss has not improved for 37 epochs.\n",
      "current in epoch    36      batch 0\n",
      "RLoss: 4123.20361328125\n",
      "current in epoch    36      batch 1\n",
      "RLoss: 409.7196960449219\n",
      "current in epoch    36      batch 2\n",
      "RLoss: 1304.910888671875\n",
      "current in epoch    36      batch 3\n",
      "RLoss: 288.5927429199219\n",
      "current in epoch    36      batch 4\n",
      "RLoss: 142.67947387695312\n",
      "current in epoch    36      batch 5\n",
      "RLoss: 260.7799377441406\n",
      "========================================\n",
      "Epoch 37/1000 - partial_train_loss: 1986.4728 \n",
      "Epoch: [37/1000], TrainLoss: 533.9894529070173\n",
      "training Loss has not improved for 38 epochs.\n",
      "current in epoch    37      batch 0\n",
      "RLoss: 697.2649536132812\n",
      "current in epoch    37      batch 1\n",
      "RLoss: 1662.7291259765625\n",
      "current in epoch    37      batch 2\n",
      "RLoss: 107.73176574707031\n",
      "current in epoch    37      batch 3\n",
      "RLoss: 1278.42919921875\n",
      "current in epoch    37      batch 4\n",
      "RLoss: 294.99395751953125\n",
      "current in epoch    37      batch 5\n",
      "RLoss: 140.73976135253906\n",
      "========================================\n",
      "Epoch 38/1000 - partial_train_loss: 1523.1385 \n",
      "Epoch: [38/1000], TrainLoss: 305.4366782052176\n",
      "training Loss has not improved for 39 epochs.\n",
      "current in epoch    38      batch 0\n",
      "RLoss: 1955.9063720703125\n",
      "current in epoch    38      batch 1\n",
      "RLoss: 127.84768676757812\n",
      "current in epoch    38      batch 2\n",
      "RLoss: 28.358097076416016\n",
      "current in epoch    38      batch 3\n",
      "RLoss: 238.97055053710938\n",
      "current in epoch    38      batch 4\n",
      "RLoss: 44.336368560791016\n",
      "current in epoch    38      batch 5\n",
      "RLoss: 3454.690673828125\n",
      "========================================\n",
      "Epoch 39/1000 - partial_train_loss: 682.4901 \n",
      "Epoch: [39/1000], TrainLoss: 5480.93314906529\n",
      "training Loss has not improved for 40 epochs.\n",
      "current in epoch    39      batch 0\n",
      "RLoss: 803.3748168945312\n",
      "current in epoch    39      batch 1\n",
      "RLoss: 837.5980834960938\n",
      "current in epoch    39      batch 2\n",
      "RLoss: 3529.269287109375\n",
      "current in epoch    39      batch 3\n",
      "RLoss: 237.9787139892578\n",
      "current in epoch    39      batch 4\n",
      "RLoss: 1090.471435546875\n",
      "current in epoch    39      batch 5\n",
      "RLoss: 346.3615417480469\n",
      "========================================\n",
      "Epoch 40/1000 - partial_train_loss: 4672.9545 \n",
      "Epoch: [40/1000], TrainLoss: 195.64353942871094\n",
      "training Loss has not improved for 41 epochs.\n",
      "current in epoch    40      batch 0\n",
      "RLoss: 121.74818420410156\n",
      "current in epoch    40      batch 1\n",
      "RLoss: 771.3516845703125\n",
      "current in epoch    40      batch 2\n",
      "RLoss: 1386.96484375\n",
      "current in epoch    40      batch 3\n",
      "RLoss: 43.270660400390625\n",
      "current in epoch    40      batch 4\n",
      "RLoss: 84.06669616699219\n",
      "current in epoch    40      batch 5\n",
      "RLoss: 325.410400390625\n",
      "========================================\n",
      "Epoch 41/1000 - partial_train_loss: 808.0411 \n",
      "sorting training set\n",
      "Epoch 41/1000 - Training loss: 445.7810 \n",
      "========================================\n",
      "Epoch: [41/1000], TrainLoss: 472.0382784118175\n",
      "training Loss has not improved for 42 epochs.\n",
      "current in epoch    41      batch 0\n",
      "RLoss: 1090.4427490234375\n",
      "current in epoch    41      batch 1\n",
      "RLoss: 185.54550170898438\n",
      "current in epoch    41      batch 2\n",
      "RLoss: 313.6890563964844\n",
      "current in epoch    41      batch 3\n",
      "RLoss: 632.7596435546875\n",
      "current in epoch    41      batch 4\n",
      "RLoss: 4855.107421875\n",
      "current in epoch    41      batch 5\n",
      "RLoss: 1051.07470703125\n",
      "========================================\n",
      "Epoch 42/1000 - partial_train_loss: 1919.3328 \n",
      "Epoch: [42/1000], TrainLoss: 1044.5257263183594\n",
      "training Loss has not improved for 43 epochs.\n",
      "current in epoch    42      batch 0\n",
      "RLoss: 624.660888671875\n",
      "current in epoch    42      batch 1\n",
      "RLoss: 612.9456176757812\n",
      "current in epoch    42      batch 2\n",
      "RLoss: 852.8971557617188\n",
      "current in epoch    42      batch 3\n",
      "RLoss: 12.688234329223633\n",
      "current in epoch    42      batch 4\n",
      "RLoss: 51.11939239501953\n",
      "current in epoch    42      batch 5\n",
      "RLoss: 11.192218780517578\n",
      "========================================\n",
      "Epoch 43/1000 - partial_train_loss: 1425.3735 \n",
      "Epoch: [43/1000], TrainLoss: 50.18930148226874\n",
      "training Loss has not improved for 44 epochs.\n",
      "current in epoch    43      batch 0\n",
      "RLoss: 479.5790100097656\n",
      "current in epoch    43      batch 1\n",
      "RLoss: 340.7232666015625\n",
      "current in epoch    43      batch 2\n",
      "RLoss: 263.06719970703125\n",
      "current in epoch    43      batch 3\n",
      "RLoss: 24.63426971435547\n",
      "current in epoch    43      batch 4\n",
      "RLoss: 40.111515045166016\n",
      "current in epoch    43      batch 5\n",
      "RLoss: 130.27684020996094\n",
      "========================================\n",
      "Epoch 44/1000 - partial_train_loss: 987.2843 \n",
      "Epoch: [44/1000], TrainLoss: 85.18450805119106\n",
      "training Loss has not improved for 45 epochs.\n",
      "current in epoch    44      batch 0\n",
      "RLoss: 857.9703369140625\n",
      "current in epoch    44      batch 1\n",
      "RLoss: 432.73077392578125\n",
      "current in epoch    44      batch 2\n",
      "RLoss: 80.45128631591797\n",
      "current in epoch    44      batch 3\n",
      "RLoss: 170.9922637939453\n",
      "current in epoch    44      batch 4\n",
      "RLoss: 1329.7099609375\n",
      "current in epoch    44      batch 5\n",
      "RLoss: 199.80902099609375\n",
      "========================================\n",
      "Epoch 45/1000 - partial_train_loss: 1489.1448 \n",
      "Epoch: [45/1000], TrainLoss: 665.9069788796561\n",
      "training Loss has not improved for 46 epochs.\n",
      "current in epoch    45      batch 0\n",
      "RLoss: 326.26837158203125\n",
      "current in epoch    45      batch 1\n",
      "RLoss: 467.1366271972656\n",
      "current in epoch    45      batch 2\n",
      "RLoss: 170.65589904785156\n",
      "current in epoch    45      batch 3\n",
      "RLoss: 1186.4541015625\n",
      "current in epoch    45      batch 4\n",
      "RLoss: 150.1141357421875\n",
      "current in epoch    45      batch 5\n",
      "RLoss: 887.6920776367188\n",
      "========================================\n",
      "Epoch 46/1000 - partial_train_loss: 1119.3430 \n",
      "sorting training set\n",
      "Epoch 46/1000 - Training loss: 811.4872 \n",
      "========================================\n",
      "Epoch: [46/1000], TrainLoss: 862.535376751265\n",
      "training Loss has not improved for 47 epochs.\n",
      "current in epoch    46      batch 0\n",
      "RLoss: 1184.113037109375\n",
      "current in epoch    46      batch 1\n",
      "RLoss: 430.02313232421875\n",
      "current in epoch    46      batch 2\n",
      "RLoss: 660.8264770507812\n",
      "current in epoch    46      batch 3\n",
      "RLoss: 206.16563415527344\n",
      "current in epoch    46      batch 4\n",
      "RLoss: 881.3970336914062\n",
      "current in epoch    46      batch 5\n",
      "RLoss: 76.69471740722656\n",
      "========================================\n",
      "Epoch 47/1000 - partial_train_loss: 1231.7675 \n",
      "Epoch: [47/1000], TrainLoss: 39.36527565547398\n",
      "training Loss has not improved for 48 epochs.\n",
      "current in epoch    47      batch 0\n",
      "RLoss: 186.22605895996094\n",
      "current in epoch    47      batch 1\n",
      "RLoss: 699.15185546875\n",
      "current in epoch    47      batch 2\n",
      "RLoss: 64.643798828125\n",
      "current in epoch    47      batch 3\n",
      "RLoss: 12.614453315734863\n",
      "current in epoch    47      batch 4\n",
      "RLoss: 179.89151000976562\n",
      "current in epoch    47      batch 5\n",
      "RLoss: 11.837362289428711\n",
      "========================================\n",
      "Epoch 48/1000 - partial_train_loss: 553.3288 \n",
      "Epoch: [48/1000], TrainLoss: 38.83943155833653\n",
      "training Loss has not improved for 49 epochs.\n",
      "current in epoch    48      batch 0\n",
      "RLoss: 18.099693298339844\n",
      "current in epoch    48      batch 1\n",
      "RLoss: 1654.201416015625\n",
      "current in epoch    48      batch 2\n",
      "RLoss: 1685.93310546875\n",
      "current in epoch    48      batch 3\n",
      "RLoss: 664.46826171875\n",
      "current in epoch    48      batch 4\n",
      "RLoss: 215.60372924804688\n",
      "current in epoch    48      batch 5\n",
      "RLoss: 815.2915649414062\n",
      "========================================\n",
      "Epoch 49/1000 - partial_train_loss: 1943.6239 \n",
      "Epoch: [49/1000], TrainLoss: 666.8216007777622\n",
      "training Loss has not improved for 50 epochs.\n",
      "current in epoch    49      batch 0\n",
      "RLoss: 32.72747039794922\n",
      "current in epoch    49      batch 1\n",
      "RLoss: 19.831256866455078\n",
      "current in epoch    49      batch 2\n",
      "RLoss: 246.4603271484375\n",
      "current in epoch    49      batch 3\n",
      "RLoss: 696.2728881835938\n",
      "current in epoch    49      batch 4\n",
      "RLoss: 113.74694061279297\n",
      "current in epoch    49      batch 5\n",
      "RLoss: 16.714553833007812\n",
      "========================================\n",
      "Epoch 50/1000 - partial_train_loss: 755.0958 \n",
      "Epoch: [50/1000], TrainLoss: 15.168168374470302\n",
      "training Loss has not improved for 51 epochs.\n",
      "current in epoch    50      batch 0\n",
      "RLoss: 49.750789642333984\n",
      "current in epoch    50      batch 1\n",
      "RLoss: 197.4499053955078\n",
      "current in epoch    50      batch 2\n",
      "RLoss: 234.4947052001953\n",
      "current in epoch    50      batch 3\n",
      "RLoss: 835.2786254882812\n",
      "current in epoch    50      batch 4\n",
      "RLoss: 161.80197143554688\n",
      "current in epoch    50      batch 5\n",
      "RLoss: 111.3173599243164\n",
      "========================================\n",
      "Epoch 51/1000 - partial_train_loss: 241.8948 \n",
      "sorting training set\n",
      "Epoch 51/1000 - Training loss: 120.0904 \n",
      "========================================\n",
      "Epoch: [51/1000], TrainLoss: 128.08014462504585\n",
      "training Loss has not improved for 52 epochs.\n",
      "current in epoch    51      batch 0\n",
      "RLoss: 2043.5167236328125\n",
      "current in epoch    51      batch 1\n",
      "RLoss: 12.6708402633667\n",
      "current in epoch    51      batch 2\n",
      "RLoss: 189.1310577392578\n",
      "current in epoch    51      batch 3\n",
      "RLoss: 1861.2957763671875\n",
      "current in epoch    51      batch 4\n",
      "RLoss: 1502.2540283203125\n",
      "current in epoch    51      batch 5\n",
      "RLoss: 1140.5408935546875\n",
      "========================================\n",
      "Epoch 52/1000 - partial_train_loss: 2449.7097 \n",
      "Epoch: [52/1000], TrainLoss: 1394.9283251081195\n",
      "training Loss has not improved for 53 epochs.\n",
      "current in epoch    52      batch 0\n",
      "RLoss: 48.51120376586914\n",
      "current in epoch    52      batch 1\n",
      "RLoss: 159.41859436035156\n",
      "current in epoch    52      batch 2\n",
      "RLoss: 200.56222534179688\n",
      "current in epoch    52      batch 3\n",
      "RLoss: 36.084388732910156\n",
      "current in epoch    52      batch 4\n",
      "RLoss: 239.29087829589844\n",
      "current in epoch    52      batch 5\n",
      "RLoss: 21.59664535522461\n",
      "========================================\n",
      "Epoch 53/1000 - partial_train_loss: 709.4079 \n",
      "Epoch: [53/1000], TrainLoss: 3.340630680322647\n",
      "training Loss has not improved for 54 epochs.\n",
      "current in epoch    53      batch 0\n",
      "RLoss: 95.10307312011719\n",
      "current in epoch    53      batch 1\n",
      "RLoss: 540.28173828125\n",
      "current in epoch    53      batch 2\n",
      "RLoss: 21.55689239501953\n",
      "current in epoch    53      batch 3\n",
      "RLoss: 501.85302734375\n",
      "current in epoch    53      batch 4\n",
      "RLoss: 176.42840576171875\n",
      "current in epoch    53      batch 5\n",
      "RLoss: 100.21285247802734\n",
      "========================================\n",
      "Epoch 54/1000 - partial_train_loss: 466.4893 \n",
      "Epoch: [54/1000], TrainLoss: 87.50731154850551\n",
      "training Loss has not improved for 55 epochs.\n",
      "current in epoch    54      batch 0\n",
      "RLoss: 47.416954040527344\n",
      "current in epoch    54      batch 1\n",
      "RLoss: 1188.12109375\n",
      "current in epoch    54      batch 2\n",
      "RLoss: 3.3723630905151367\n",
      "current in epoch    54      batch 3\n",
      "RLoss: 3.8746185302734375\n",
      "current in epoch    54      batch 4\n",
      "RLoss: 395.4267883300781\n",
      "current in epoch    54      batch 5\n",
      "RLoss: 185.39772033691406\n",
      "========================================\n",
      "Epoch 55/1000 - partial_train_loss: 546.9988 \n",
      "Epoch: [55/1000], TrainLoss: 210.72751889910018\n",
      "training Loss has not improved for 56 epochs.\n",
      "current in epoch    55      batch 0\n",
      "RLoss: 222.64227294921875\n",
      "current in epoch    55      batch 1\n",
      "RLoss: 288.16619873046875\n",
      "current in epoch    55      batch 2\n",
      "RLoss: 302.28240966796875\n",
      "current in epoch    55      batch 3\n",
      "RLoss: 182.88421630859375\n",
      "current in epoch    55      batch 4\n",
      "RLoss: 529.9631958007812\n",
      "current in epoch    55      batch 5\n",
      "RLoss: 33.486961364746094\n",
      "========================================\n",
      "Epoch 56/1000 - partial_train_loss: 560.0037 \n",
      "sorting training set\n",
      "Epoch 56/1000 - Training loss: 36.2431 \n",
      "========================================\n",
      "Epoch: [56/1000], TrainLoss: 39.71195865351398\n",
      "training Loss has not improved for 57 epochs.\n",
      "current in epoch    56      batch 0\n",
      "RLoss: 61.61572265625\n",
      "current in epoch    56      batch 1\n",
      "RLoss: 299.06732177734375\n",
      "current in epoch    56      batch 2\n",
      "RLoss: 176.27041625976562\n",
      "current in epoch    56      batch 3\n",
      "RLoss: 24.170944213867188\n",
      "current in epoch    56      batch 4\n",
      "RLoss: 346.57611083984375\n",
      "current in epoch    56      batch 5\n",
      "RLoss: 74.77398681640625\n",
      "========================================\n",
      "Epoch 57/1000 - partial_train_loss: 244.6913 \n",
      "Epoch: [57/1000], TrainLoss: 63.9027327810015\n",
      "training Loss has not improved for 58 epochs.\n",
      "current in epoch    57      batch 0\n",
      "RLoss: 14.482209205627441\n",
      "current in epoch    57      batch 1\n",
      "RLoss: 80.06487274169922\n",
      "current in epoch    57      batch 2\n",
      "RLoss: 661.6049194335938\n",
      "current in epoch    57      batch 3\n",
      "RLoss: 263.238525390625\n",
      "current in epoch    57      batch 4\n",
      "RLoss: 444.88690185546875\n",
      "current in epoch    57      batch 5\n",
      "RLoss: 639.7257690429688\n",
      "========================================\n",
      "Epoch 58/1000 - partial_train_loss: 543.1126 \n",
      "Epoch: [58/1000], TrainLoss: 809.8110068184989\n",
      "training Loss has not improved for 59 epochs.\n",
      "current in epoch    58      batch 0\n",
      "RLoss: 304.2748107910156\n",
      "current in epoch    58      batch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFT_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43madv_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_target\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtheta_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattack_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_gamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_flag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m rtheta \u001b[38;5;241m=\u001b[39m method\u001b[38;5;241m.\u001b[39mr([[adv_data, adv_target]], alpha\u001b[38;5;241m=\u001b[39mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     58\u001b[0m method\u001b[38;5;241m.\u001b[39mtheta_grad \u001b[38;5;241m=\u001b[39m grad(rtheta, \u001b[38;5;28mlist\u001b[39m(method\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters()), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[73], line 141\u001b[0m, in \u001b[0;36mStableAL.train_theta\u001b[0;34m(self, data, epochs_theta, epoch_attack, gamma, end_flag, epoch, batch_idx)\u001b[0m\n\u001b[1;32m    138\u001b[0m     dtheta_dx \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dloss_dtheta\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 141\u001b[0m         dtheta_dx\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdloss_dtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxa_grad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(dtheta_dx, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m#print(f\"xa_grad size: {self.xa_grad.shape}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "min_weight = torch.min(method.weights)\n",
    "attack_gamma = (1.0 / min_weight).data\n",
    "criterion = nn.MSELoss()\n",
    "epoch = 0\n",
    "zero_list = []\n",
    "end_flag = False\n",
    "\n",
    "method.model =IRNet_intorch(150).to(device)\n",
    "method.model.optimizer = optim.Adam(method.model.parameters(), lr=config_dict['network_lr'])\n",
    "# Create a dictionary to store the best loss for each dataset\n",
    "best_loss_dict = {}\n",
    "all_loss_dict = {}\n",
    "\n",
    "while epoch <=config_dict['num_epochs']:\n",
    "    train_loss = 0.0\n",
    "\n",
    "    #Xshift_tsne_mse_loss = 0\n",
    "    #Xshift_umap_mse_loss = 0\n",
    "    #statY_delta_e_mse_loss = 0\n",
    "    #infoY_delta_e_mse_loss = 0\n",
    "    #statY_bandgap_mse_loss = 0\n",
    "    #infoY_bandgap_mse_loss = 0\n",
    "    #Rsplt_testset1_mse_loss = 0\n",
    "    #Rsplt_testset2_mse_loss = 0\n",
    "    #Rsplt_testset3_mse_loss = 0\n",
    "    #Rsplt_testset4_mse_loss = 0\n",
    "    #Rsplt_testset5_mse_loss = 0\n",
    "    #piezo_test_mse_loss = 0\n",
    "\n",
    "    partial_train_loss = 0.0\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    minima = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(f\"current in epoch    {epoch}      batch {batch_idx}\")\n",
    "\n",
    "         # Zero the gradients\n",
    "        method.model.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs.to(device) , target.float().to(device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        method.model.optimizer.step()\n",
    "        partial_train_loss += loss.cpu().item()\n",
    "        if epoch <config_dict['FT_epoch']:\n",
    "            continue\n",
    "\n",
    "        method.train_theta((adv_data, adv_target), config_dict['theta_epoch'], config_dict['attack_epoch'], attack_gamma, end_flag, epoch, batch_idx)\n",
    "        rtheta = method.r([[adv_data, adv_target]], alpha=config_dict['alpha'] / math.sqrt(epoch + 1))\n",
    "        method.theta_grad = grad(rtheta, list(method.model.parameters()), create_graph=True, allow_unused=True)\n",
    "        dr_dx = torch.matmul(method.theta_grad[config_dict['grad_layer']].reshape(-1), method.xa_grad).squeeze()\n",
    "        deltaw = dr_dx * method.weight_grad\n",
    "        deltaw = torch.sum(deltaw, 0)\n",
    "\n",
    "        deltaw[zero_list] = 0.0\n",
    "        max_grad = torch.max(torch.abs(deltaw))\n",
    "        deltastep = config_dict['deltaall']\n",
    "        lr_weight = (deltastep / max_grad).detach()\n",
    "        print(f'RLoss: {rtheta.data}')\n",
    "\n",
    "        if epoch >config_dict['partial_epoch_start']:\n",
    "\n",
    "            if batch_idx ==config_dict['partial_batch']:# train partial trainset\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    partial_train_loss /= (batch_idx+1)\n",
    "    print(\"==\"*20)\n",
    "    print(f\"Epoch {epoch+1}/{config_dict['num_epochs']} - partial_train_loss: {partial_train_loss:.4f} \")\n",
    "    if epoch %config_dict['partial_epoch_every']==0:\n",
    "        with torch.no_grad():\n",
    "                print(\"sorting training set\")\n",
    "                # for sorting Training set\n",
    "                sort_MAE=[]\n",
    "                method.model.eval()\n",
    "                for i in range(len(train_dataset.inputs)):\n",
    "                    inp = train_dataset.inputs[i]\n",
    "                    tar = train_dataset.labels[i]\n",
    "\n",
    "                    x = torch.tensor([inp.tolist()], dtype=torch.float32).to(device)\n",
    "                    y = torch.tensor(tar.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "                    output = method.model(x)\n",
    "                    loss = criterion(output, y).cpu()\n",
    "                    # Accumulate the training loss\n",
    "                    train_loss += loss.item()\n",
    "                    #print(f\"loss:       {loss}\")\n",
    "                    sort_MAE.append({'data' : inp,\n",
    "                                                'label' :tar,\n",
    "                                                'loss' : loss})\n",
    "\n",
    "\n",
    "                if epoch%config_dict['partial_epoch_save']==0:\n",
    "                    sort_MAE = pd.DataFrame(sort_MAE)\n",
    "                    sort_MAE.to_csv(config_dict['exp-dir'] / 'sorted/train_set_sorting_{epoch}.csv',index=False)\n",
    "\n",
    "                sort_MAE = pd.DataFrame(sort_MAE)\n",
    "                new_train = sort_MAE.sort_values(by=['loss'],ascending=False)\n",
    "                new_train_dataset = RecurrentDataset(new_train)\n",
    "                train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "\n",
    "                train_loss /= len(train_dataset.inputs)\n",
    "                print(f\"Epoch {epoch+1}/{config_dict['num_epochs']} - Training loss: {train_loss:.4f} \")\n",
    "                print(\"==\"*20)\n",
    "\n",
    "\n",
    "    mse_losses_dict = {}\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    print(f'Epoch: [{(epoch + 1)}/{ config_dict[\"num_epochs\"] }], TrainLoss: {train_loss}')\n",
    "\n",
    "    mse_losses_dict[\"Train\"] = train_loss\n",
    "    mse_losses_dict[\"Partial_train\"] = partial_train_loss\n",
    "\n",
    "    def calculate_mse_loss(data_loader, model, criterion, device):\n",
    "        mse_loss = 0.0\n",
    "        total_batches = len(data_loader)\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            outputs = model(data.to(device))\n",
    "            loss = criterion(outputs, target.float().to(device))\n",
    "            mse_loss += loss.item()\n",
    "\n",
    "        mse_loss /= total_batches\n",
    "        return mse_loss\n",
    "\n",
    "\n",
    "    data_loaders = {\n",
    "         # 'Xshift_tsne': Xshift_tsne_loader, 'Xshift_umap': Xshift_umap_loader,\n",
    "                # 'statY_delta_e': statY_delta_e_loader, 'infoY_delta_e': infoY_delta_e_loader,\n",
    "                # 'statY_bandgap': statY_bandgap_loader, 'infoY_bandgap': infoY_bandgap_loader,\n",
    "                'Rsplt_testset1': Rsplt_testset1_loader, 'Rsplt_testset2': Rsplt_testset2_loader,\n",
    "                'Rsplt_testset3': Rsplt_testset3_loader, 'Rsplt_testset4': Rsplt_testset4_loader,\n",
    "                'Rsplt_testset5': Rsplt_testset5_loader, 'piezo_test': piezo1_test_loader}\n",
    "\n",
    "    # Iterate over each data loader and calculate MSE loss\n",
    "    for dataset_name, loader in data_loaders.items():\n",
    "        loader_mse_loss = calculate_mse_loss(loader, method.model, criterion, device)\n",
    "        # Append the MSE loss to the list in the dictionary\n",
    "        mse_losses_dict[dataset_name] = loader_mse_loss\n",
    "\n",
    "\n",
    "    rsplt_ave = np.average([\n",
    "                                mse_losses_dict[\"Rsplt_testset1\"],\n",
    "                                mse_losses_dict[\"Rsplt_testset2\"],\n",
    "                                mse_losses_dict[\"Rsplt_testset3\"],\n",
    "                                mse_losses_dict[\"Rsplt_testset4\"],\n",
    "                                mse_losses_dict[\"Rsplt_testset5\"],\n",
    "\n",
    "                                ])\n",
    "    mse_losses_dict[\"rsplt_ave\"] = rsplt_ave\n",
    "\n",
    "    save =[]\n",
    "    # Separate loop to update the best loss for all datasets\n",
    "    for dataset_name, loss in mse_losses_dict.items():\n",
    "        if dataset_name not in best_loss_dict or loader_mse_loss < best_loss_dict[dataset_name]:\n",
    "            best_loss_dict[dataset_name] = loader_mse_loss\n",
    "            save.append(dataset_name)\n",
    "\n",
    "\n",
    "    # Stop the training process if the training loss has stopped decreasing or has started to increase\n",
    "    if train_loss < train_best_loss:\n",
    "        train_best_loss = train_loss\n",
    "        counter = 0\n",
    "        torch.save(method.model, config_dict['exp-dir'] / f'models/IR3_epoch_{epoch}.pt')\n",
    "        torch.save(method.weights, config_dict['exp-dir'] / 'models/SAL_weight_{epoch}_gamma_{attack_gamma}.pt')\n",
    "        save.append(\"Train\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f'training Loss has not improved for {counter} epochs.')\n",
    "\n",
    "\n",
    "\n",
    "    if rsplt_ave < Rsplt_ave_best_loss:\n",
    "            Rsplt_ave_best_loss  =  rsplt_ave\n",
    "            counter_val = 0\n",
    "            torch.save(method.model,config_dict['exp-dir'] / \"models/IR3_SAL-bset-Rsplt_test_mse_loss.pt\")\n",
    "            save.append(\"Rsplt_AVE\")\n",
    "    else:\n",
    "        counter_val += 1\n",
    "        if counter_val >= config_dict['early_stop']:\n",
    "            print(f'Training stopped. Valid (rand) Loss has not improved for {500} epochs.')\n",
    "            break\n",
    "\n",
    "    all_loss_dict[epoch + 1] = mse_losses_dict\n",
    "\n",
    "    mse_losses_df = pd.DataFrame.from_dict(all_loss_dict, orient='index')\n",
    "\n",
    "\n",
    "    mse_losses_df.reset_index().rename(columns={'index': 'epoch'}).to_feather(config_dict['exp-dir']/ 'SAL-training_loss.feather')\n",
    "    epoch=epoch+1\n",
    "\n",
    "    # adjust gamma according to min(weight)\n",
    "\n",
    "    for i in range(method.weights.shape[0]):\n",
    "        if method.weights[i] > 0.0 and method.weights[i] < min_weight:\n",
    "            min_weight = method.weights[i]\n",
    "        if method.weights[i] < 0.0:\n",
    "            method.weights[i] = 1.0\n",
    "            zero_list.append(i)\n",
    "\n",
    "    attack_gamma = (1.0 / min_weight).data\n",
    "    if epoch <=config_dict['FT_epoch']:\n",
    "        continue\n",
    "\n",
    "    method.weights -= lr_weight * deltaw.detach().reshape(method.weights.shape)\n",
    "    del rtheta\n",
    "    del dr_dx\n",
    "    del deltaw\n",
    "    del max_grad\n",
    "    del deltastep\n",
    "    del lr_weight\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(method.weights,  config_dict['exp-dir'] / f'Whole_SAL_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(method.model,  config_dict['exp-dir'] / f'IR3_epoch_{epoch}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('OOD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9527c1aa2febaf9c7bab479ad701127eaf7cbb2c19ab2b26ad381c63904c9a00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
