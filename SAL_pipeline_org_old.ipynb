{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import gc\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(vector):\n",
    "    if type(vector) is list:\n",
    "        vlist = vector\n",
    "    elif type(vector) is np.ndarray:\n",
    "        vlist = vector.reshape(-1).tolist()\n",
    "    else:\n",
    "        vlist = vector.view(-1).tolist()\n",
    "\n",
    "    return \"[\" + \", \".join(\"{:+.4f}\".format(vi) for vi in vlist) + \"]\"\n",
    "\n",
    "# Optimizer Class to maximize loss of adversarial dataset\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=1e-3, beta1=0.9, beta2=0.9, epsilon=1e-8):\n",
    "        self.device = torch.device(device)\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.m_hat = None\n",
    "        self.v_hat = None\n",
    "        self.initialize = False\n",
    "\n",
    "    # Grad = adv_gradient\n",
    "    # Iternum = iteration\n",
    "    # theta = adv_images\n",
    "    # Gradient ascent\n",
    "    def update(self, grad, iternum, theta):\n",
    "        if not self.initialize:\n",
    "            self.m = (1 - self.beta1) * grad\n",
    "            self.v = (1 - self.beta2) * grad ** 2\n",
    "            self.initialize = True\n",
    "        else:\n",
    "            assert self.m.shape == grad.shape\n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * grad ** 2\n",
    "\n",
    "        self.m_hat = self.m / (1 - self.beta1 ** iternum)\n",
    "        self.v_hat = self.v / (1 - self.beta2 ** iternum)\n",
    "        return theta + self.lr * self.m_hat / (self.epsilon + torch.sqrt(self.v_hat))\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "def save_config(config, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)\n",
    "\n",
    "# Load the configuration from the YAML file\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tensor\n",
    "with open('feat_col_name.txt', 'r') as file:\n",
    "    column_names = file.read().split()\n",
    "def save_tensor(ori_data,ori_lab,adv_data,adv_lab):\n",
    "    ori_data_list  = ori_data.clone().cpu().numpy()\n",
    "    ori_labels_list = ori_lab.clone().cpu().numpy()\n",
    "    adv_data_list  = adv_data.clone().cpu().numpy()\n",
    "    adv_labels_list = adv_lab.clone().cpu().tolist()\n",
    "\n",
    "\n",
    "    csv_filename = f'{config_dict[\"exp-dir\"]}adv/adv_data_labels_in_attack{epoch}_{batch_idx}.csv'\n",
    "\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write header\n",
    "        header = column_names+ ['label']#[f'feature_{i}' for i in range(ori_data_list.shape[1])]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write data and labels for each group\n",
    "        rows_group1 = np.column_stack((ori_data_list, ori_labels_list))\n",
    "        rows_group2 = np.column_stack((adv_data_list, ori_labels_list))\n",
    "\n",
    "\n",
    "        # Combine data from the first two groups only\n",
    "        all_rows = np.vstack((rows_group1, rows_group2))\n",
    "\n",
    "        writer.writerows(all_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRNet_intorch(torch.nn.Module):\n",
    "    #'128-64-16'\n",
    "    def __init__(self, input_size):\n",
    "        super(IRNet_intorch, self).__init__()\n",
    "        self.fc128 =nn.Linear(128, 128)\n",
    "        self.fc64 =nn.Linear(64, 64)\n",
    "        self.fc16 =nn.Linear(16, 16)\n",
    "\n",
    "        self.bn128 =nn.BatchNorm1d(128)\n",
    "        self.bn64 =nn.BatchNorm1d(64)\n",
    "        self.bn16 =nn.BatchNorm1d(16)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.inputlayer = nn.Linear(input_size, 128)\n",
    "\n",
    "        self.con128_64 = nn.Linear(128, 64)\n",
    "        self.con64_16 = nn.Linear(64,16)\n",
    "        self.output16 = nn.Linear(16,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inputlayer(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc128(x)\n",
    "        x = self.bn128(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "        x = self.con128_64(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc64(x)\n",
    "        x = self.bn64(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "        x = self.con64_16(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc16(x)\n",
    "        x = self.bn16(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "\n",
    "        x = self.output16(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create folder 'Exp_02-02--18:07'.\n",
      "Subfolder 'models' already exists.\n",
      "Subfolder 'sorted' already exists.\n",
      "Subfolder 'adv' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_folder_based_on_time():\n",
    "    # Get the current time\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    # Format the time as a string with only date, hour, and minute (e.g., \"2024-01-23_12-30\")\n",
    "    time_str = current_time.strftime(\"%m-%d--%H:%M\")\n",
    "\n",
    "    # Create a folder name based on the formatted time\n",
    "    folder_name = f\"Exp_{time_str}\"\n",
    "\n",
    "\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(folder_name):\n",
    "        # Create the folder\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"Folder '{folder_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to create folder '{folder_name}'.\")\n",
    "\n",
    "\n",
    "    # Create a subfolder called \"models\" within the main folder\n",
    "    models_folder = os.path.join(folder_name, 'models')\n",
    "\n",
    "    # Check if the \"models\" subfolder exists\n",
    "    if not os.path.exists(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "        print(f\"Subfolder 'models' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Subfolder 'models' already exists.\")\n",
    "\n",
    "    # Create a subfolder called \"models\" within the main folder\n",
    "    models_folder = os.path.join(folder_name, 'sorted')\n",
    "\n",
    "    # Check if the \"models\" subfolder exists\n",
    "    if not os.path.exists(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "        print(f\"Subfolder 'sorted' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Subfolder 'sorted' already exists.\")\n",
    "    # Create a subfolder called \"models\" within the main folder\n",
    "    models_folder = os.path.join(folder_name, 'adv')\n",
    "        # Check if the \"models\" subfolder exists\n",
    "    if not os.path.exists(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "        print(f\"Subfolder 'adv' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Subfolder 'adv' already exists.\")\n",
    "\n",
    "\n",
    "    return folder_name\n",
    "\n",
    "# Example usage\n",
    "created_folder = create_folder_based_on_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exp_02-02--18:07'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best_loss      = float('inf')\n",
    "partial_train_best_loss= float('inf')\n",
    "Rsplt_test_best_loss = float('inf')\n",
    "Xshft_test_best_loss = float('inf')\n",
    "pizeo_test_best_loss = float('inf')\n",
    "statY_test_best_loss = float('inf')\n",
    "infoY_test_best_loss = float('inf')\n",
    "\n",
    "Rsplt_testset1_best_loss = float('inf')\n",
    "Rsplt_testset2_best_loss = float('inf')\n",
    "Rsplt_testset3_best_loss = float('inf')\n",
    "Rsplt_testset4_best_loss = float('inf')\n",
    "Rsplt_testset5_best_loss = float('inf')\n",
    "loss_df = pd.DataFrame(columns=[\n",
    "                        'epoch',\n",
    "                        'train',\n",
    "                        'partialtrain',\n",
    "                        'Rsplt1',\n",
    "                        'Rsplt2',\n",
    "                        'Rsplt3',\n",
    "                        'Rsplt4',\n",
    "                        'Rsplt5',\n",
    "                        'RspltAVE',\n",
    "                        'Xshft',\n",
    "                        'pizeo',\n",
    "                        'statY',\n",
    "                        'infoY',\n",
    "                        'BLANK',\n",
    "                        'best_train',\n",
    "                        'best_partialtrain',\n",
    "                        'best_Rsplt1',\n",
    "                        'best_Rsplt2',\n",
    "                        'best_Rsplt3',\n",
    "                        'best_Rsplt4',\n",
    "                        'best_Rsplt5',\n",
    "                        'best_Rsplt_AVE',\n",
    "                        'best_Xshft',\n",
    "                        'best_pizeo',\n",
    "                        'best_statY',\n",
    "                        'best_infoY',\n",
    "                        'save',\n",
    "                        'attack_gamma'\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'exp-dir':f'./{created_folder}/',\n",
    "\n",
    "    # Number of covariates\n",
    "    'dim_x': 150,\n",
    "    #func r\n",
    "    'alpha_for_r':10,\n",
    "    #func attack\n",
    "    'attack_lr': 7e-2,\n",
    "    #func train_theta\n",
    "    'opt_lr':0.01,\n",
    "    'advadv_rate':5,\n",
    "    'grad_layer': 4,\n",
    "    'xa_grad_reduce':-0.01,\n",
    "\n",
    "    #dataset\n",
    "    'batch_size':128,\n",
    "    #SAL\n",
    "    'deltaall':20,\n",
    "    'alpha' :0.5,\n",
    "\n",
    "    'theta_epoch':10,\n",
    "    'attack_epoch':2,\n",
    "\n",
    "    'min_weight':1e8,\n",
    "\n",
    "    #network\n",
    "    'network_lr':0.01,\n",
    "    'num_epochs':1000,\n",
    "\n",
    "    #full training set though network no SAL\n",
    "    'FT_epoch':4,\n",
    "\n",
    "    #partial training starts at and num of batch used\n",
    "    'partial_epoch_start':4,\n",
    "    'partial_batch':5,\n",
    "    'partial_epoch_every':5,\n",
    "    'partial_epoch_save':50,\n",
    "\n",
    "\n",
    "    'early_stop':500,\n",
    "\n",
    "    #for adv data store\n",
    "    'current_epoch':0,\n",
    "    'current_batch':0,\n",
    "    'current_theta_epoch':0,\n",
    "    'current_attack_epoch':0,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Configuration: {'FT_epoch': 4, 'advadv_rate': 5, 'alpha': 0.5, 'alpha_for_r': 10, 'attack_epoch': 2, 'attack_lr': 0.07, 'batch_size': 128, 'current_attack_epoch': 0, 'current_batch': 0, 'current_epoch': 0, 'current_theta_epoch': 0, 'deltaall': 20, 'dim_x': 150, 'early_stop': 500, 'exp-dir': './Exp_02-02--18:07/', 'grad_layer': 4, 'min_weight': 100000000.0, 'network_lr': 0.01, 'num_epochs': 1000, 'opt_lr': 0.01, 'partial_batch': 5, 'partial_epoch_every': 5, 'partial_epoch_save': 50, 'partial_epoch_start': 4, 'theta_epoch': 10, 'xa_grad_reduce': -0.01}\n"
     ]
    }
   ],
   "source": [
    "save_config(config_dict, f'{config_dict[\"exp-dir\"]}config.yaml')\n",
    "loaded_config = load_config(f'{config_dict[\"exp-dir\"]}config.yaml')\n",
    "print(\"Loaded Configuration:\", loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableAL():\n",
    "    def __init__(self, environment):\n",
    "        self.weights = None\n",
    "        self.model = None\n",
    "        self.weight_grad = None\n",
    "        self.xa_grad = None\n",
    "        self.theta_grad = None\n",
    "        self.gamma = None\n",
    "        self.adversarial_data = None\n",
    "        self.loss_criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.adv_based_on = None\n",
    "        self.adv_again = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        # init\n",
    "        # Number of covariates\n",
    "        dim_x=config_dict['dim_x']\n",
    "\n",
    "        self.model = IRNet_intorch(dim_x).to(device)\n",
    "        # Covariate Weights\n",
    "        self.weights = torch.zeros(dim_x).reshape(-1, 1) + 100.0\n",
    "        self.weights = self.weights.to(device)\n",
    "\n",
    "    def cost_function(self, x, x_adv):\n",
    "        # Variable cost level where the weights determine the cost level\n",
    "        cost = torch.mean(((x - x_adv) ** 2).mm(self.weights)).to(device)\n",
    "        return cost\n",
    "\n",
    "    # Loss across Training environments\n",
    "    # Self.loss_criterion = MSELoss\n",
    "    def r(self, environments, alpha=config_dict['alpha_for_r']):\n",
    "        result = 0.0\n",
    "        env_loss = []\n",
    "        for x_e, y_e in environments:\n",
    "            x_e =x_e.to(device)\n",
    "            y_e =y_e.to(torch.float32).to(device)\n",
    "            env_loss.append(self.loss_criterion(self.model(x_e), y_e))\n",
    "        env_loss = torch.Tensor(env_loss)\n",
    "        max_index = torch.argmax(env_loss)\n",
    "        min_index = torch.argmin(env_loss)\n",
    "\n",
    "        for idx, (x_e, y_e) in enumerate(environments):\n",
    "            x_e =x_e.to(device)\n",
    "            y_e =y_e.to(torch.float32).to(device)\n",
    "            if idx == max_index:\n",
    "                result += (alpha+1)*self.loss_criterion(self.model(x_e), y_e)\n",
    "            elif idx == min_index:\n",
    "                result += (1-alpha)*self.loss_criterion(self.model(x_e), y_e)\n",
    "            else:\n",
    "                result += self.loss_criterion(self.model(x_e),y_e)\n",
    "        return result\n",
    "\n",
    "\n",
    "    # generate adversarial data\n",
    "    # Maximize the loss using their own ADAM.update method(their own optimizer)\n",
    "    def attack(self, gamma, data, step):\n",
    "        attack_lr = config_dict['attack_lr']\n",
    "        images, labels = data\n",
    "        images_adv = images.clone().detach()\n",
    "\n",
    "        optimizer = Adam(learning_rate=attack_lr)\n",
    "\n",
    "        for i in range(step):\n",
    "            if images_adv.grad is not None:\n",
    "                images_adv.grad.data.zero_()\n",
    "\n",
    "\n",
    "            images_adv=images_adv.to(device)\n",
    "            images_adv.requires_grad_(True)\n",
    "            outputs = self.model(images_adv)\n",
    "\n",
    "            labels = labels.float().to(device)\n",
    "            images = images.to(device)\n",
    "            loss = self.loss_criterion(\n",
    "                outputs, labels) - gamma * self.cost_function(images, images_adv)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            images_adv.data = optimizer.update(images_adv.grad, i + 1, images_adv)\n",
    "\n",
    "        self.weight_grad = -2 * gamma * attack_lr * (images_adv - images)\n",
    "        temp_image = images_adv.clone().detach()\n",
    "        temp_label = labels.clone().detach()\n",
    "        self.adversarial_data = (temp_image, temp_label)\n",
    "\n",
    "        #save adv and ori data\n",
    "        save_tensor(images,labels,temp_image,temp_label)\n",
    "\n",
    "        return images_adv, labels\n",
    "\n",
    "\n",
    "    # Optimizes the model paremeters such that the loss is minimized\n",
    "    # on the adversarial data from self.attack\n",
    "    def train_theta(self, data, epochs_theta, epoch_attack, gamma, end_flag=False):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=config_dict['opt_lr'])\n",
    "        self.adv_based_on = data\n",
    "        # For __ Theta epochs_theta\n",
    "        for i_theta in range(epochs_theta):\n",
    "            if i_theta % config_dict['advadv_rate'] == 0 or not end_flag:\n",
    "                images_adv, labels = self.attack(gamma, data, step=epoch_attack)\n",
    "\n",
    "            else:\n",
    "                self.adv_again = self.adversarial_data\n",
    "                images_adv, labels = self.attack(gamma, self.adversarial_data, step=epoch_attack)\n",
    "\n",
    "\n",
    "            #print(f\"original data: {data[0].shape}\")\n",
    "            # print(f\"attack data: {images_adv.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            images_adv =images_adv.to(device)\n",
    "            outputs = self.model(images_adv)\n",
    "            loss = self.loss_criterion(outputs, labels.float())\n",
    "\n",
    "\n",
    "\n",
    "            if self.xa_grad is None:\n",
    "                dtheta_dx = []\n",
    "                dloss_dtheta = grad(loss, self.model.parameters(), create_graph=True)[config_dict['grad_layer']].reshape(-1)\n",
    "\n",
    "                # size dloss = model.para size\n",
    "                for name1, param in self.model.named_parameters():\n",
    "                    print (f\"grad: {name1}          {param.shape}\")\n",
    "\n",
    "                #time.sleep(5.5)    # Pause 5.5 seconds\n",
    "                print(f\"dloss_dtheta.shape:  {dloss_dtheta.shape[0]}\")\n",
    "                for j in range(dloss_dtheta.shape[0]):\n",
    "                    #print(f\"dloss_dtheta.shape[0]:j     {j}\")\n",
    "                    dtheta_dx.append(grad(dloss_dtheta[j], images_adv, create_graph=True)[0].detach())\n",
    "\n",
    "                self.xa_grad = torch.stack(dtheta_dx,1).detach()\n",
    "\n",
    "            else:\n",
    "                dloss_dtheta = grad(loss, self.model.parameters(), create_graph=True)[config_dict['grad_layer']].reshape(-1)\n",
    "                dtheta_dx = []\n",
    "\n",
    "                for j in range(dloss_dtheta.shape[0]):\n",
    "                    dtheta_dx.append(grad(dloss_dtheta[j], images_adv, create_graph=True)[0].detach())\n",
    "                self.xa_grad += torch.stack(dtheta_dx, 1).detach()\n",
    "\n",
    "            #print(f\"xa_grad size: {self.xa_grad.shape}\")\n",
    "            del dtheta_dx\n",
    "            del dloss_dtheta\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            #print('%d | %.4f | %s'%(i, loss, pretty(self.model.layer[4].weight)))\n",
    "            #if i % 1000 == 999:\n",
    "\n",
    "\n",
    "            #print(f\"loss?\")\n",
    "            loss.backward(retain_graph=True)\n",
    "            #print(f\"step?\")\n",
    "            optimizer.step()\n",
    "            #print(f\"step!\")\n",
    "        self.xa_grad *= config_dict['xa_grad_reduce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the CSV file\n",
    "\n",
    "Rsplt_testset =pd.read_csv('./dataset/Rsplt_testset.csv', index_col=None)\n",
    "Xshft_testset =pd.read_csv('./dataset/Xshft_testset.csv', index_col=None)\n",
    "pizeo_testset =pd.read_csv('./dataset/pizeo_testset.csv', index_col=None)\n",
    "statY_testset =pd.read_csv('./dataset/statY_testset.csv', index_col=None)\n",
    "infoY_testset =pd.read_csv('./dataset/infoY_testset.csv', index_col=None)\n",
    "final_train   =pd.read_csv('./dataset/final_trainset.csv',index_col=None)\n",
    "\n",
    "Rsplt_testset1 =pd.read_csv('./dataset/Rsplt_testset1.csv', index_col=None)\n",
    "Rsplt_testset2 =pd.read_csv('./dataset/Rsplt_testset2.csv', index_col=None)\n",
    "Rsplt_testset3 =pd.read_csv('./dataset/Rsplt_testset3.csv', index_col=None)\n",
    "Rsplt_testset4 =pd.read_csv('./dataset/Rsplt_testset4.csv', index_col=None)\n",
    "Rsplt_testset5 =pd.read_csv('./dataset/Rsplt_testset5.csv', index_col=None)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define a custom PyTorch Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df.drop(columns=['delta_e','pretty_comp']).values\n",
    "        self.labels = df['delta_e'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index].tolist()[:]\n",
    "        label = self.labels[index].tolist()\n",
    "        return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "    def getSALdata(self):\n",
    "        input = np.array(self.inputs[:].tolist())\n",
    "        label = np.array(self.labels[:].tolist())\n",
    "        return (input, label)\n",
    "\n",
    "class RecurrentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['data'].values\n",
    "        self.labels = df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index].tolist()[:]\n",
    "        label = self.labels[index].tolist()\n",
    "        return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Step 4: Use DataLoader to create batches\n",
    "batch_size = config_dict['batch_size']\n",
    "\n",
    "train_dataset = MyDataset(final_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "Rsplt_test_dataset = MyDataset(Rsplt_testset)\n",
    "Xshft_test_dataset = MyDataset(Xshft_testset)\n",
    "pizeo_test_dataset = MyDataset(pizeo_testset)\n",
    "statY_test_dataset = MyDataset(statY_testset)\n",
    "infoY_test_dataset = MyDataset(infoY_testset)\n",
    "\n",
    "Rsplt_testset1_dataset = MyDataset(Rsplt_testset1)\n",
    "Rsplt_testset2_dataset = MyDataset(Rsplt_testset2)\n",
    "Rsplt_testset3_dataset = MyDataset(Rsplt_testset3)\n",
    "Rsplt_testset4_dataset = MyDataset(Rsplt_testset4)\n",
    "Rsplt_testset5_dataset = MyDataset(Rsplt_testset5)\n",
    "\n",
    "Rsplt_testset1_loader = DataLoader(Rsplt_testset1_dataset, batch_size=len(Rsplt_testset1))\n",
    "Rsplt_testset2_loader = DataLoader(Rsplt_testset2_dataset, batch_size=len(Rsplt_testset2))\n",
    "Rsplt_testset3_loader = DataLoader(Rsplt_testset3_dataset, batch_size=len(Rsplt_testset3))\n",
    "Rsplt_testset4_loader = DataLoader(Rsplt_testset4_dataset, batch_size=len(Rsplt_testset4))\n",
    "Rsplt_testset5_loader = DataLoader(Rsplt_testset5_dataset, batch_size=len(Rsplt_testset5))\n",
    "\n",
    "Rsplt_test_loader = DataLoader(Rsplt_test_dataset, batch_size=len(Rsplt_testset))\n",
    "Xshft_test_loader = DataLoader(Xshft_test_dataset, batch_size=len(Xshft_testset))\n",
    "pizeo_test_loader = DataLoader(pizeo_test_dataset, batch_size=len(pizeo_testset))\n",
    "statY_test_loader = DataLoader(statY_test_dataset, batch_size=len(statY_testset))\n",
    "infoY_test_loader = DataLoader(infoY_test_dataset, batch_size=len(infoY_testset))\n",
    "\n",
    "\n",
    "data=train_dataset.getSALdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = StableAL([train_dataset.getSALdata()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current in epoch    0      batch 0\n",
      "current in epoch    0      batch 1\n",
      "current in epoch    0      batch 2\n",
      "current in epoch    0      batch 3\n",
      "current in epoch    0      batch 4\n",
      "current in epoch    0      batch 5\n",
      "current in epoch    0      batch 6\n",
      "current in epoch    0      batch 7\n",
      "current in epoch    0      batch 8\n",
      "current in epoch    0      batch 9\n",
      "current in epoch    0      batch 10\n",
      "current in epoch    0      batch 11\n",
      "current in epoch    0      batch 12\n",
      "current in epoch    0      batch 13\n",
      "current in epoch    0      batch 14\n",
      "current in epoch    0      batch 15\n",
      "current in epoch    0      batch 16\n",
      "current in epoch    0      batch 17\n",
      "current in epoch    0      batch 18\n",
      "current in epoch    0      batch 19\n",
      "current in epoch    0      batch 20\n",
      "current in epoch    0      batch 21\n",
      "current in epoch    0      batch 22\n",
      "current in epoch    0      batch 23\n",
      "current in epoch    0      batch 24\n",
      "current in epoch    0      batch 25\n",
      "current in epoch    0      batch 26\n",
      "current in epoch    0      batch 27\n",
      "========================================\n",
      "Epoch 1/1000 - partial_train_loss: 15296.7498 \n",
      "sorting training set\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1366058/218610080.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0;31m# Accumulate the training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0;31m#print(f\"loss:       {loss}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     sort_MAE = sort_MAE.append({'data' : inp,\n\u001b[0m\u001b[1;32m    135\u001b[0m                                                 \u001b[0;34m'label'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                                                 'loss' : loss},\n\u001b[1;32m    137\u001b[0m                                                 ignore_index = True)\n",
      "\u001b[0;32m~/anaconda3/envs/baysic/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6289\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6290\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6291\u001b[0m         ):\n\u001b[1;32m   6292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "min_weight = torch.min(method.weights)\n",
    "attack_gamma = (1.0 / min_weight).data\n",
    "criterion = nn.MSELoss()\n",
    "epoch = 0\n",
    "zero_list = []\n",
    "end_flag = False\n",
    "\n",
    "method.model =IRNet_intorch(config_dict['dim_x']).to(device)\n",
    "method.model.optimizer = optim.Adam(method.model.parameters(), lr=config_dict['network_lr'])\n",
    "\n",
    "while epoch <=config_dict['num_epochs']:\n",
    "    train_loss = 0.0\n",
    "    Rsplt1_test_mse_loss = 0\n",
    "    Rsplt2_test_mse_loss = 0\n",
    "    Rsplt3_test_mse_loss = 0\n",
    "    Rsplt4_test_mse_loss = 0\n",
    "    Rsplt5_test_mse_loss = 0\n",
    "    Rsplt_test_mse_loss = 0\n",
    "    Xshft_test_mse_loss = 0\n",
    "    pizeo_test_mse_loss = 0\n",
    "    statY_test_mse_loss = 0\n",
    "    infoY_test_mse_loss = 0\n",
    "    partial_train_loss = 0.0\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    minima = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(f\"current in epoch    {epoch}      batch {batch_idx}\")\n",
    "\n",
    "         # Zero the gradients\n",
    "        method.model.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs.to(device) , target.float().to(device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        method.model.optimizer.step()\n",
    "        partial_train_loss += loss.cpu().item()\n",
    "        if epoch <config_dict['FT_epoch']:\n",
    "            continue\n",
    "\n",
    "        method.train_theta((data, target), config_dict['theta_epoch'], config_dict['attack_epoch'], attack_gamma, end_flag)\n",
    "        rtheta = method.r([[data, target]], alpha=config_dict['alpha'] / math.sqrt(epoch + 1))\n",
    "        method.theta_grad = grad(rtheta, list(method.model.parameters()), create_graph=True, allow_unused=True)\n",
    "        dr_dx = torch.matmul(method.theta_grad[config_dict['grad_layer']].reshape(-1), method.xa_grad).squeeze()\n",
    "        deltaw = dr_dx * method.weight_grad\n",
    "        deltaw = torch.sum(deltaw, 0)\n",
    "\n",
    "        deltaw[zero_list] = 0.0\n",
    "        max_grad = torch.max(torch.abs(deltaw))\n",
    "        deltastep = config_dict['deltaall']\n",
    "        lr_weight = (deltastep / max_grad).detach()\n",
    "        print(f'RLoss: {rtheta.data}')\n",
    "\n",
    "\n",
    "        '''\n",
    "        if epoch %50==0:\n",
    "            #save adv ori advadv data\n",
    "            images, labels = method.adversarial_data\n",
    "            adv_data_list = data.numpy()\n",
    "            #adv_labels_list = labels.tolist()\n",
    "\n",
    "            images, labels = method.adv_based_on\n",
    "            ori_data_list = data.numpy()\n",
    "            ori_labels_list = labels.numpy()\n",
    "\n",
    "            #images, labels = method.adv_again\n",
    "            #advadv_data_list = data.numpy()\n",
    "            #advadv_labels_list = labels.tolist()\n",
    "\n",
    "\n",
    "            csv_filename = f'adv_data_labels_{epoch}_{batch_idx}.csv' if method.adv_again is None else f'adv_adv_data_labels_{epoch}_{batch_idx}.csv'\n",
    "\n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "\n",
    "                # Write header\n",
    "                header = [f'feature_{i}' for i in range(ori_data_list.shape[1])] + ['label']\n",
    "                writer.writerow(header)\n",
    "\n",
    "                # Write data and labels for each group\n",
    "                rows_group1 = np.column_stack((ori_data_list, ori_labels_list))\n",
    "                rows_group2 = np.column_stack((adv_data_list, ori_labels_list))\n",
    "\n",
    "                # Check if data_group3 is not None before including it\n",
    "                if method.adv_again is not None:\n",
    "                    images, labels = method.adv_again\n",
    "                    advadv_data_list = data.numpy()\n",
    "                    rows_group3 = np.column_stack((advadv_data_list, ori_labels_list))\n",
    "\n",
    "                    # Combine data from all groups\n",
    "                    all_rows = np.vstack((rows_group1, rows_group2, rows_group3))\n",
    "                else:\n",
    "                    # Combine data from the first two groups only\n",
    "                    all_rows = np.vstack((rows_group1, rows_group2))\n",
    "\n",
    "                writer.writerows(all_rows)\n",
    "        '''\n",
    "        if epoch >config_dict['partial_epoch_start']:\n",
    "\n",
    "            if batch_idx ==config_dict['partial_batch']:# train partial trainset\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    partial_train_loss /= (batch_idx+1)\n",
    "    print(\"==\"*20)\n",
    "    print(f\"Epoch {epoch+1}/{config_dict['num_epochs']} - partial_train_loss: {partial_train_loss:.4f} \")\n",
    "    if epoch %config_dict['partial_epoch_every']==0:\n",
    "        with torch.no_grad():\n",
    "                print(\"sorting training set\")\n",
    "                # for sorting Training set\n",
    "                sort_MAE=pd.DataFrame(columns = ['data', 'label', 'loss'])\n",
    "                method.model.eval()\n",
    "                for i in range(len(train_dataset.inputs)):\n",
    "                    inp = train_dataset.inputs[i]\n",
    "                    tar = train_dataset.labels[i]\n",
    "\n",
    "                    x = torch.tensor([inp.tolist()], dtype=torch.float32).to(device)\n",
    "                    y = torch.tensor(tar.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "                    output = method.model(x)\n",
    "                    loss = criterion(output, y).cpu()\n",
    "                    # Accumulate the training loss\n",
    "                    train_loss += loss.item()\n",
    "                    #print(f\"loss:       {loss}\")\n",
    "                    sort_MAE = sort_MAE.append({'data' : inp,\n",
    "                                                'label' :tar,\n",
    "                                                'loss' : loss},\n",
    "                                                ignore_index = True)\n",
    "\n",
    "\n",
    "                if epoch%config_dict['partial_epoch_save']==0:\n",
    "                    sort_MAE.to_csv(f\"{config_dict['exp-dir']}sorted/train_set_sorting_{epoch}.csv\",index=False)\n",
    "                new_train = sort_MAE.sort_values(by=['loss'],ascending=False)\n",
    "                new_train_dataset = RecurrentDataset(new_train)\n",
    "                train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "\n",
    "                train_loss /= len(train_dataset.inputs)\n",
    "                print(f\"Epoch {epoch+1}/{config_dict['num_epochs']} - Training loss: {train_loss:.4f} \")\n",
    "                print(\"==\"*20)\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    print(f'Epoch: [{(epoch + 1)}/{ config_dict[\"num_epochs\"] }], TrainLoss: {train_loss}')\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset1_loader):\n",
    "\n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt1_test_mse_loss += loss.item()\n",
    "    Rsplt1_test_mse_loss /= len(Rsplt_testset1_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset2_loader):\n",
    "\n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt2_test_mse_loss += loss.item()\n",
    "    Rsplt2_test_mse_loss /= len(Rsplt_testset2_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset3_loader):\n",
    "\n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt3_test_mse_loss += loss.item()\n",
    "    Rsplt3_test_mse_loss /= len(Rsplt_testset3_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset4_loader):\n",
    "\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt4_test_mse_loss += loss.item()\n",
    "    Rsplt4_test_mse_loss /= len(Rsplt_testset4_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset5_loader):\n",
    "\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt5_test_mse_loss += loss.item()\n",
    "    Rsplt5_test_mse_loss /= len(Rsplt_testset5_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_test_loader):\n",
    "\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt_test_mse_loss += loss.item()\n",
    "    Rsplt_test_mse_loss /= len(Rsplt_test_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Xshft_test_loader):\n",
    "\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Xshft_test_mse_loss += loss.item()\n",
    "    Xshft_test_mse_loss /= len(Xshft_test_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pizeo_test_loader):\n",
    "\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        pizeo_test_mse_loss += loss.item()\n",
    "    pizeo_test_mse_loss /= len(pizeo_test_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(statY_test_loader):\n",
    "\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        statY_test_mse_loss += loss.item()\n",
    "    statY_test_mse_loss /= len(statY_test_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(infoY_test_loader):\n",
    "\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        infoY_test_mse_loss += loss.item()\n",
    "    infoY_test_mse_loss /= len(infoY_test_loader)\n",
    "\n",
    "\n",
    "    rsplt_ave = np.average([\n",
    "                                Rsplt1_test_mse_loss,\n",
    "                                Rsplt2_test_mse_loss,\n",
    "                                Rsplt3_test_mse_loss,\n",
    "                                Rsplt4_test_mse_loss,\n",
    "                                Rsplt5_test_mse_loss\n",
    "                                ])\n",
    "\n",
    "    save =[]\n",
    "    if partial_train_loss < partial_train_best_loss:\n",
    "        partial_train_best_loss = partial_train_loss\n",
    "        save.append(\"partialTrain\")\n",
    "    if Rsplt1_test_mse_loss < Rsplt_testset1_best_loss:\n",
    "            Rsplt_testset1_best_loss  =  Rsplt1_test_mse_loss\n",
    "            save.append(\"Rsplt1\")\n",
    "    if Rsplt2_test_mse_loss < Rsplt_testset2_best_loss:\n",
    "            Rsplt_testset2_best_loss  =  Rsplt2_test_mse_loss\n",
    "            save.append(\"Rsplt2\")\n",
    "    if Rsplt3_test_mse_loss < Rsplt_testset3_best_loss:\n",
    "            Rsplt_testset3_best_loss  =  Rsplt3_test_mse_loss\n",
    "            save.append(\"Rsplt3\")\n",
    "    if Rsplt4_test_mse_loss < Rsplt_testset4_best_loss:\n",
    "            Rsplt_testset4_best_loss  =  Rsplt4_test_mse_loss\n",
    "            save.append(\"Rsplt4\")\n",
    "    if Rsplt5_test_mse_loss < Rsplt_testset5_best_loss:\n",
    "            Rsplt_testset5_best_loss  =  Rsplt5_test_mse_loss\n",
    "            save.append(\"Rsplt5\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if Xshft_test_mse_loss < Xshft_test_best_loss:\n",
    "            Xshft_test_best_loss  =  Xshft_test_mse_loss\n",
    "            save.append(\"Xshft\")\n",
    "    if pizeo_test_mse_loss < pizeo_test_best_loss:\n",
    "            pizeo_test_best_loss  =  pizeo_test_mse_loss\n",
    "            save.append(\"pizeo\")\n",
    "    if statY_test_mse_loss < statY_test_best_loss:\n",
    "            statY_test_best_loss  =  statY_test_mse_loss\n",
    "            save.append(\"statY\")\n",
    "    if infoY_test_mse_loss < infoY_test_best_loss:\n",
    "            infoY_test_best_loss  =  infoY_test_mse_loss\n",
    "            save.append(\"infoY\")\n",
    "\n",
    "\n",
    "    # Stop the training process if the training loss has stopped decreasing or has started to increase\n",
    "    if train_loss < train_best_loss:\n",
    "        train_best_loss = train_loss\n",
    "        counter = 0\n",
    "        torch.save(method.model, f'{config_dict[\"exp-dir\"]}models/IR3_epoch_{epoch}.pt')\n",
    "        torch.save(method.weights, f\"{config_dict['exp-dir']}models/SAL_weight_{epoch}_gamma_{attack_gamma}.pt\")\n",
    "        save.append(\"Train\")\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "        print(f'training Loss has not improved for {counter} epochs.')\n",
    "\n",
    "\n",
    "\n",
    "    if rsplt_ave < Rsplt_test_best_loss:\n",
    "            Rsplt_test_best_loss  =  rsplt_ave\n",
    "            counter_val = 0\n",
    "            torch.save(method.model,f\"{config_dict['exp-dir']}models/IR3_SAL-bset-Rsplt_test_mse_loss.pt\")\n",
    "            save.append(\"Rsplt_AVE\")\n",
    "    else:\n",
    "        counter_val += 1\n",
    "        if counter_val >= config_dict['early_stop']:\n",
    "            print(f'Training stopped. Valid (rand) Loss has not improved for {500} epochs.')\n",
    "            break\n",
    "\n",
    "\n",
    "    entry = [epoch,\n",
    "                                    f\"{train_loss:.4f}\",\n",
    "                                    f\"{partial_train_loss:.4f}\",\n",
    "                                    f\"{Rsplt1_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt2_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt3_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt4_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt5_test_mse_loss:.4f}\",\n",
    "                                    f\"{rsplt_ave:.4f}\",\n",
    "\n",
    "                                    f\"{Xshft_test_mse_loss:.4f}\",\n",
    "                                    f\"{pizeo_test_mse_loss:.4f}\",\n",
    "                                    f\"{statY_test_mse_loss:.4f}\",\n",
    "                                    f\"{infoY_test_mse_loss:.4f}\",\n",
    "\n",
    "                                    f\"                         \",\n",
    "\n",
    "                                    f\"{train_best_loss:.4f}\",\n",
    "                                    f\"{partial_train_best_loss:.4f}\",\n",
    "                                    f\"{Rsplt_testset1_best_loss:.4f}\",\n",
    "                                    f\"{Rsplt_testset2_best_loss:.4f}\",\n",
    "                                    f\"{Rsplt_testset3_best_loss:.4f}\",\n",
    "                                    f\"{Rsplt_testset4_best_loss:.4f}\",\n",
    "                                    f\"{Rsplt_testset5_best_loss:.4f}\",\n",
    "\n",
    "                                    f\"{Rsplt_test_best_loss:.4f}\",\n",
    "                                    f\"{Xshft_test_best_loss:.4f}\",\n",
    "                                    f\"{pizeo_test_best_loss:.4f}\",\n",
    "                                    f\"{statY_test_best_loss:.4f}\",\n",
    "                                    f\"{infoY_test_best_loss:.4f}\",\n",
    "\n",
    "                                    save    ,\n",
    "                                    f'{attack_gamma}'\n",
    "                                    ]\n",
    "    loss_df.loc[len(loss_df)] = entry\n",
    "\n",
    "    loss_df.to_csv(config_dict['exp-dir']+'SAL-training_loss.csv', index=False)\n",
    "    epoch=epoch+1\n",
    "\n",
    "    # adjust gamma according to min(weight)\n",
    "\n",
    "    for i in range(method.weights.shape[0]):\n",
    "        if method.weights[i] > 0.0 and method.weights[i] < min_weight:\n",
    "            min_weight = method.weights[i]\n",
    "        if method.weights[i] < 0.0:\n",
    "            method.weights[i] = 1.0\n",
    "            zero_list.append(i)\n",
    "\n",
    "    attack_gamma = (1.0 / min_weight).data\n",
    "    if epoch <=config_dict['FT_epoch']:\n",
    "        continue\n",
    "\n",
    "    method.weights -= lr_weight * deltaw.detach().reshape(method.weights.shape)\n",
    "    del rtheta\n",
    "    del dr_dx\n",
    "    del deltaw\n",
    "    del max_grad\n",
    "    del deltastep\n",
    "    del lr_weight\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(method.weights, f\"Whole_SAL_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(method.model, f\"IR3_epoch_{epoch}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('OOD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9527c1aa2febaf9c7bab479ad701127eaf7cbb2c19ab2b26ad381c63904c9a00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
