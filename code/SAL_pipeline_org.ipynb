{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import gc\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(vector):\n",
    "    if type(vector) is list:\n",
    "        vlist = vector\n",
    "    elif type(vector) is np.ndarray:\n",
    "        vlist = vector.reshape(-1).tolist()\n",
    "    else:\n",
    "        vlist = vector.view(-1).tolist()\n",
    "\n",
    "    return \"[\" + \", \".join(\"{:+.4f}\".format(vi) for vi in vlist) + \"]\"\n",
    "\n",
    "# Optimizer Class to maximize loss of adversarial dataset\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=1e-3, beta1=0.9, beta2=0.9, epsilon=1e-8):\n",
    "        self.device = torch.device(device)\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.m_hat = None\n",
    "        self.v_hat = None\n",
    "        self.initialize = False\n",
    "\n",
    "    # Grad = adv_gradient\n",
    "    # Iternum = iteration\n",
    "    # theta = adv_images\n",
    "    # Gradient ascent\n",
    "    def update(self, grad, iternum, theta):\n",
    "        if not self.initialize:\n",
    "            self.m = (1 - self.beta1) * grad\n",
    "            self.v = (1 - self.beta2) * grad ** 2\n",
    "            self.initialize = True\n",
    "        else:\n",
    "            assert self.m.shape == grad.shape\n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * grad ** 2\n",
    "\n",
    "        self.m_hat = self.m / (1 - self.beta1 ** iternum)\n",
    "        self.v_hat = self.v / (1 - self.beta2 ** iternum)\n",
    "        return theta + self.lr * self.m_hat / (self.epsilon + torch.sqrt(self.v_hat))\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "def save_config(config, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)\n",
    "\n",
    "# Load the configuration from the YAML file\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tensor\n",
    "with open('feat_col_name.txt', 'r') as file:\n",
    "    column_names = file.read().split()\n",
    "def save_tensor(ori_data,ori_lab,adv_data,adv_lab):\n",
    "    ori_data_list  = ori_data.clone().cpu().numpy()\n",
    "    ori_labels_list = ori_lab.clone().cpu().numpy()  \n",
    "    adv_data_list  = adv_data.clone().cpu().numpy()\n",
    "    adv_labels_list = adv_lab.clone().cpu().tolist()\n",
    "  \n",
    "        \n",
    "    csv_filename = f'{config_dict[\"exp-dir\"]}adv/adv_data_labels_in_attack{epoch}_{batch_idx}.csv' \n",
    "        \n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "    \n",
    "        # Write header\n",
    "        header = column_names+ ['label']#[f'feature_{i}' for i in range(ori_data_list.shape[1])] \n",
    "        writer.writerow(header)\n",
    "    \n",
    "        # Write data and labels for each group\n",
    "        rows_group1 = np.column_stack((ori_data_list, ori_labels_list))\n",
    "        rows_group2 = np.column_stack((adv_data_list, ori_labels_list))\n",
    "    \n",
    "        \n",
    "        # Combine data from the first two groups only\n",
    "        all_rows = np.vstack((rows_group1, rows_group2))\n",
    "    \n",
    "        writer.writerows(all_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRNet_intorch(torch.nn.Module):\n",
    "    #'128-64-16'    \n",
    "    def __init__(self, input_size):\n",
    "        super(IRNet_intorch, self).__init__()\n",
    "        self.fc128 =nn.Linear(128, 128)\n",
    "        self.fc64 =nn.Linear(64, 64)\n",
    "        self.fc16 =nn.Linear(16, 16)\n",
    "\n",
    "        self.bn128 =nn.BatchNorm1d(128)\n",
    "        self.bn64 =nn.BatchNorm1d(64)\n",
    "        self.bn16 =nn.BatchNorm1d(16)\n",
    "      \n",
    "        self.relu = nn.ReLU()\n",
    "        self.inputlayer = nn.Linear(input_size, 128)\n",
    "\n",
    "        self.con128_64 = nn.Linear(128, 64)\n",
    "        self.con64_16 = nn.Linear(64,16)\n",
    "        self.output16 = nn.Linear(16,1)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.inputlayer(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc128(x)\n",
    "        x = self.bn128(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "        x = self.con128_64(x)\n",
    "    \n",
    "        x_res = x\n",
    "        x = self.fc64(x)\n",
    "        x = self.bn64(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res\n",
    "        x = self.con64_16(x)\n",
    "\n",
    "        x_res = x\n",
    "        x = self.fc16(x)\n",
    "        x = self.bn16(x)\n",
    "        x = self.relu(x)\n",
    "        x = x+x_res   \n",
    "\n",
    "        x = self.output16(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'Exp_01-23--17:45' created successfully.\n",
      "Subfolder 'models' created successfully.\n",
      "Subfolder 'sorted' created successfully.\n",
      "Subfolder 'adv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_folder_based_on_time():\n",
    "    # Get the current time\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    # Format the time as a string with only date, hour, and minute (e.g., \"2024-01-23_12-30\")\n",
    "    time_str = current_time.strftime(\"%m-%d--%H:%M\")\n",
    "\n",
    "    # Create a folder name based on the formatted time\n",
    "    folder_name = f\"Exp_{time_str}\"\n",
    "\n",
    "\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(folder_name):\n",
    "        # Create the folder\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"Folder '{folder_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to create folder '{folder_name}'.\")\n",
    "\n",
    "\n",
    "    # Create a subfolder called \"models\" within the main folder\n",
    "    models_folder = os.path.join(folder_name, 'models')\n",
    "\n",
    "    # Check if the \"models\" subfolder exists\n",
    "    if not os.path.exists(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "        print(f\"Subfolder 'models' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Subfolder 'models' already exists.\")\n",
    "\n",
    "    # Create a subfolder called \"models\" within the main folder\n",
    "    models_folder = os.path.join(folder_name, 'sorted')\n",
    "\n",
    "    # Check if the \"models\" subfolder exists\n",
    "    if not os.path.exists(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "        print(f\"Subfolder 'sorted' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Subfolder 'sorted' already exists.\")\n",
    "    # Create a subfolder called \"models\" within the main folder\n",
    "    models_folder = os.path.join(folder_name, 'adv')\n",
    "        # Check if the \"models\" subfolder exists\n",
    "    if not os.path.exists(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "        print(f\"Subfolder 'adv' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Subfolder 'adv' already exists.\")\n",
    "\n",
    "\n",
    "    return folder_name\n",
    "\n",
    "# Example usage\n",
    "created_folder = create_folder_based_on_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exp_01-23--17:45'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best_loss      = float('inf')\n",
    "partial_train_best_loss= float('inf')\n",
    "Rsplt_test_best_loss = float('inf')\n",
    "Xshft_test_best_loss = float('inf')\n",
    "pizeo_test_best_loss = float('inf')\n",
    "statY_test_best_loss = float('inf')\n",
    "infoY_test_best_loss = float('inf')\n",
    "\n",
    "Rsplt_testset1_best_loss = float('inf')\n",
    "Rsplt_testset2_best_loss = float('inf')\n",
    "Rsplt_testset3_best_loss = float('inf')\n",
    "Rsplt_testset4_best_loss = float('inf')\n",
    "Rsplt_testset5_best_loss = float('inf')\n",
    "loss_df = pd.DataFrame(columns=[\n",
    "                        'epoch', \n",
    "                        'train',\n",
    "                        'partialtrain',\n",
    "                        'Rsplt1',\n",
    "                        'Rsplt2',\n",
    "                        'Rsplt3',\n",
    "                        'Rsplt4',\n",
    "                        'Rsplt5',\n",
    "                        'RspltAVE',\n",
    "                        'Xshft',\n",
    "                        'pizeo',\n",
    "                        'statY',\n",
    "                        'infoY',\n",
    "                        'BLANK',\n",
    "                        'best_train',\n",
    "                        'best_partialtrain',\n",
    "                        'best_Rsplt1',\n",
    "                        'best_Rsplt2',\n",
    "                        'best_Rsplt3',\n",
    "                        'best_Rsplt4',\n",
    "                        'best_Rsplt5',\n",
    "                        'best_Rsplt_AVE',\n",
    "                        'best_Xshft',\n",
    "                        'best_pizeo',\n",
    "                        'best_statY',\n",
    "                        'best_infoY',\n",
    "                        'save',\n",
    "                        'attack_gamma'\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'exp-dir':f'./{created_folder}/',\n",
    "    \n",
    "    # Number of covariates\n",
    "    'dim_x': 150, \n",
    "    #func r   \n",
    "    'alpha_for_r':10,\n",
    "    #func attack\n",
    "    'attack_lr': 7e-2,\n",
    "    #func train_theta\n",
    "    'opt_lr':0.01,\n",
    "    'advadv_rate':5,\n",
    "    'grad_layer': 4,\n",
    "    'xa_grad_reduce':-0.01,\n",
    "\n",
    "    #dataset\n",
    "    'batch_size':128,\n",
    "    #SAL\n",
    "    'deltaall':20,\n",
    "    'alpha' :0.5,\n",
    "\n",
    "    'theta_epoch':10,\n",
    "    'attack_epoch':2, \n",
    "\n",
    "    'min_weight':1e8,\n",
    "\n",
    "    #network\n",
    "    'network_lr':0.01,\n",
    "    'num_epochs':1000,\n",
    "\n",
    "    #full training set though network no SAL\n",
    "    'FT_epoch':4,\n",
    "\n",
    "    #partial training starts at and num of batch used\n",
    "    'partial_epoch_start':4,\n",
    "    'partial_batch':5,\n",
    "    'partial_epoch_every':5,\n",
    "    'partial_epoch_save':50,\n",
    "    \n",
    "\n",
    "    'early_stop':500,\n",
    "\n",
    "    #for adv data store\n",
    "    'current_epoch':0,\n",
    "    'current_batch':0,\n",
    "    'current_theta_epoch':0,\n",
    "    'current_attack_epoch':0,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Configuration: {'FT_epoch': 4, 'advadv_rate': 5, 'alpha': 0.5, 'alpha_for_r': 10, 'attack_epoch': 2, 'attack_lr': 0.07, 'batch_size': 128, 'current_attack_epoch': 0, 'current_batch': 0, 'current_epoch': 0, 'current_theta_epoch': 0, 'deltaall': 20, 'dim_x': 150, 'early_stop': 500, 'exp-dir': './Exp_01-23--17:45/', 'grad_layer': 4, 'min_weight': 100000000.0, 'network_lr': 0.01, 'num_epochs': 1000, 'opt_lr': 0.01, 'partial_batch': 5, 'partial_epoch_every': 5, 'partial_epoch_save': 50, 'partial_epoch_start': 4, 'theta_epoch': 10, 'xa_grad_reduce': -0.01}\n"
     ]
    }
   ],
   "source": [
    "save_config(config_dict, f'{config_dict[\"exp-dir\"]}config.yaml')\n",
    "loaded_config = load_config(f'{config_dict[\"exp-dir\"]}config.yaml')\n",
    "print(\"Loaded Configuration:\", loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableAL():\n",
    "    def __init__(self, environment):\n",
    "        self.weights = None\n",
    "        self.model = None\n",
    "        self.weight_grad = None\n",
    "        self.xa_grad = None\n",
    "        self.theta_grad = None\n",
    "        self.gamma = None\n",
    "        self.adversarial_data = None\n",
    "        self.loss_criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.adv_based_on = None\n",
    "        self.adv_again = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "       \n",
    "        # init\n",
    "        # Number of covariates\n",
    "        dim_x=config_dict['dim_x']\n",
    "\n",
    "        self.model = IRNet_intorch(dim_x).to(device)\n",
    "        # Covariate Weights\n",
    "        self.weights = torch.zeros(dim_x).reshape(-1, 1) + 100.0\n",
    "        self.weights = self.weights.to(device)\n",
    "        \n",
    "    def cost_function(self, x, x_adv):\n",
    "        # Variable cost level where the weights determine the cost level\n",
    "        cost = torch.mean(((x - x_adv) ** 2).mm(self.weights)).to(device)\n",
    "        return cost\n",
    "\n",
    "    # Loss across Training environments\n",
    "    # Self.loss_criterion = MSELoss\n",
    "    def r(self, environments, alpha=config_dict['alpha_for_r']):\n",
    "        result = 0.0\n",
    "        env_loss = []\n",
    "        for x_e, y_e in environments:\n",
    "            x_e =x_e.to(device)\n",
    "            y_e =y_e.to(torch.float32).to(device)\n",
    "            env_loss.append(self.loss_criterion(self.model(x_e), y_e))\n",
    "        env_loss = torch.Tensor(env_loss)\n",
    "        max_index = torch.argmax(env_loss)\n",
    "        min_index = torch.argmin(env_loss)\n",
    "\n",
    "        for idx, (x_e, y_e) in enumerate(environments):\n",
    "            x_e =x_e.to(device)\n",
    "            y_e =y_e.to(torch.float32).to(device)\n",
    "            if idx == max_index:\n",
    "                result += (alpha+1)*self.loss_criterion(self.model(x_e), y_e)\n",
    "            elif idx == min_index:\n",
    "                result += (1-alpha)*self.loss_criterion(self.model(x_e), y_e)\n",
    "            else:\n",
    "                result += self.loss_criterion(self.model(x_e),y_e)\n",
    "        return result\n",
    "\n",
    "  \n",
    "    # generate adversarial data\n",
    "    # Maximize the loss using their own ADAM.update method(their own optimizer)\n",
    "    def attack(self, gamma, data, step):\n",
    "        attack_lr = config_dict['attack_lr']\n",
    "        images, labels = data\n",
    "        images_adv = images.clone().detach()\n",
    "        \n",
    "        optimizer = Adam(learning_rate=attack_lr)\n",
    "\n",
    "        for i in range(step):\n",
    "            if images_adv.grad is not None:\n",
    "                images_adv.grad.data.zero_()\n",
    "\n",
    "\n",
    "            images_adv=images_adv.to(device)\n",
    "            images_adv.requires_grad_(True)\n",
    "            outputs = self.model(images_adv)\n",
    "           \n",
    "            labels = labels.float().to(device)\n",
    "            images = images.to(device)\n",
    "            loss = self.loss_criterion(\n",
    "                outputs, labels) - gamma * self.cost_function(images, images_adv)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            images_adv.data = optimizer.update(images_adv.grad, i + 1, images_adv)\n",
    "\n",
    "        self.weight_grad = -2 * gamma * attack_lr * (images_adv - images)\n",
    "        temp_image = images_adv.clone().detach()\n",
    "        temp_label = labels.clone().detach()\n",
    "        self.adversarial_data = (temp_image, temp_label)\n",
    "\n",
    "        #save adv and ori data\n",
    "        save_tensor(images,labels,temp_image,temp_label)\n",
    "       \n",
    "        return images_adv, labels\n",
    "\n",
    "   \n",
    "    # Optimizes the model paremeters such that the loss is minimized\n",
    "    # on the adversarial data from self.attack\n",
    "    def train_theta(self, data, epochs_theta, epoch_attack, gamma, end_flag=False):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=config_dict['opt_lr'])\n",
    "        self.adv_based_on = data\n",
    "        # For __ Theta epochs_theta\n",
    "        for i_theta in range(epochs_theta):\n",
    "            if i_theta % config_dict['advadv_rate'] == 0 or not end_flag:\n",
    "                images_adv, labels = self.attack(gamma, data, step=epoch_attack)\n",
    "\n",
    "            else:\n",
    "                self.adv_again = self.adversarial_data\n",
    "                images_adv, labels = self.attack(gamma, self.adversarial_data, step=epoch_attack)\n",
    "            \n",
    "                \n",
    "            #print(f\"original data: {data[0].shape}\")\n",
    "            # print(f\"attack data: {images_adv.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            images_adv =images_adv.to(device)\n",
    "            outputs = self.model(images_adv)\n",
    "            loss = self.loss_criterion(outputs, labels.float()) \n",
    "            \n",
    "\n",
    "            \n",
    "            if self.xa_grad is None:\n",
    "                dtheta_dx = []\n",
    "                dloss_dtheta = grad(loss, self.model.parameters(), create_graph=True)[config_dict['grad_layer']].reshape(-1)\n",
    "\n",
    "                # size dloss = model.para size\n",
    "                for name1, param in self.model.named_parameters():\n",
    "                    print (f\"grad: {name1}          {param.shape}\")\n",
    "\n",
    "                #time.sleep(5.5)    # Pause 5.5 seconds\n",
    "                print(f\"dloss_dtheta.shape:  {dloss_dtheta.shape[0]}\")\n",
    "                for j in range(dloss_dtheta.shape[0]):\n",
    "                    #print(f\"dloss_dtheta.shape[0]:j     {j}\")\n",
    "                    dtheta_dx.append(grad(dloss_dtheta[j], images_adv, create_graph=True)[0].detach()) \n",
    "   \n",
    "                self.xa_grad = torch.stack(dtheta_dx,1).detach()\n",
    "                \n",
    "            else:\n",
    "                dloss_dtheta = grad(loss, self.model.parameters(), create_graph=True)[config_dict['grad_layer']].reshape(-1)\n",
    "                dtheta_dx = []\n",
    "\n",
    "                for j in range(dloss_dtheta.shape[0]):\n",
    "                    dtheta_dx.append(grad(dloss_dtheta[j], images_adv, create_graph=True)[0].detach())\n",
    "                self.xa_grad += torch.stack(dtheta_dx, 1).detach()\n",
    "                \n",
    "            #print(f\"xa_grad size: {self.xa_grad.shape}\")\n",
    "            del dtheta_dx\n",
    "            del dloss_dtheta\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            #print('%d | %.4f | %s'%(i, loss, pretty(self.model.layer[4].weight)))\n",
    "            #if i % 1000 == 999:\n",
    "              \n",
    "\n",
    "            #print(f\"loss?\")\n",
    "            loss.backward(retain_graph=True)\n",
    "            #print(f\"step?\")\n",
    "            optimizer.step()\n",
    "            #print(f\"step!\")\n",
    "        self.xa_grad *= config_dict['xa_grad_reduce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the CSV file\n",
    "\n",
    "Rsplt_testset =pd.read_csv('../dataset/Rsplt_testset.csv', index_col=None)\n",
    "Xshft_testset =pd.read_csv('../dataset/Xshft_testset.csv', index_col=None)\n",
    "pizeo_testset =pd.read_csv('../dataset/pizeo_testset.csv', index_col=None)\n",
    "statY_testset =pd.read_csv('../dataset/statY_testset.csv', index_col=None)\n",
    "infoY_testset =pd.read_csv('../dataset/infoY_testset.csv', index_col=None)\n",
    "final_train   =pd.read_csv('../dataset/final_trainset.csv',index_col=None)  \n",
    "\n",
    "Rsplt_testset1 =pd.read_csv('../dataset/Rsplt_testset1.csv', index_col=None)\n",
    "Rsplt_testset2 =pd.read_csv('../dataset/Rsplt_testset2.csv', index_col=None)\n",
    "Rsplt_testset3 =pd.read_csv('../dataset/Rsplt_testset3.csv', index_col=None)\n",
    "Rsplt_testset4 =pd.read_csv('../dataset/Rsplt_testset4.csv', index_col=None)\n",
    "Rsplt_testset5 =pd.read_csv('../dataset/Rsplt_testset5.csv', index_col=None)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define a custom PyTorch Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df.drop(columns=['delta_e','pretty_comp']).values\n",
    "        self.labels = df['delta_e'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index].tolist()[:]\n",
    "        label = self.labels[index].tolist()\n",
    "        return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "    def getSALdata(self):\n",
    "        input = np.array(self.inputs[:].tolist())\n",
    "        label = np.array(self.labels[:].tolist())\n",
    "        return (input, label)\n",
    "        \n",
    "class RecurrentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['data'].values\n",
    "        self.labels = df['label'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input = self.inputs[index].tolist()[:]\n",
    "        label = self.labels[index].tolist()\n",
    "        return torch.tensor(input, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Step 4: Use DataLoader to create batches\n",
    "batch_size = config_dict['batch_size']\n",
    "\n",
    "train_dataset = MyDataset(final_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "Rsplt_test_dataset = MyDataset(Rsplt_testset)\n",
    "Xshft_test_dataset = MyDataset(Xshft_testset)\n",
    "pizeo_test_dataset = MyDataset(pizeo_testset)\n",
    "statY_test_dataset = MyDataset(statY_testset)\n",
    "infoY_test_dataset = MyDataset(infoY_testset)\n",
    "\n",
    "Rsplt_testset1_dataset = MyDataset(Rsplt_testset1)\n",
    "Rsplt_testset2_dataset = MyDataset(Rsplt_testset2)\n",
    "Rsplt_testset3_dataset = MyDataset(Rsplt_testset3)\n",
    "Rsplt_testset4_dataset = MyDataset(Rsplt_testset4)\n",
    "Rsplt_testset5_dataset = MyDataset(Rsplt_testset5)\n",
    "\n",
    "Rsplt_testset1_loader = DataLoader(Rsplt_testset1_dataset, batch_size=len(Rsplt_testset1))\n",
    "Rsplt_testset2_loader = DataLoader(Rsplt_testset2_dataset, batch_size=len(Rsplt_testset2))\n",
    "Rsplt_testset3_loader = DataLoader(Rsplt_testset3_dataset, batch_size=len(Rsplt_testset3))\n",
    "Rsplt_testset4_loader = DataLoader(Rsplt_testset4_dataset, batch_size=len(Rsplt_testset4))\n",
    "Rsplt_testset5_loader = DataLoader(Rsplt_testset5_dataset, batch_size=len(Rsplt_testset5))\n",
    "\n",
    "Rsplt_test_loader = DataLoader(Rsplt_test_dataset, batch_size=len(Rsplt_testset))\n",
    "Xshft_test_loader = DataLoader(Xshft_test_dataset, batch_size=len(Xshft_testset))\n",
    "pizeo_test_loader = DataLoader(pizeo_test_dataset, batch_size=len(pizeo_testset))\n",
    "statY_test_loader = DataLoader(statY_test_dataset, batch_size=len(statY_testset))\n",
    "infoY_test_loader = DataLoader(infoY_test_dataset, batch_size=len(infoY_testset))\n",
    "\n",
    "\n",
    "data=train_dataset.getSALdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = StableAL([train_dataset.getSALdata()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current in epoch    0      batch 0\n",
      "current in epoch    0      batch 1\n",
      "current in epoch    0      batch 2\n",
      "current in epoch    0      batch 3\n",
      "current in epoch    0      batch 4\n",
      "current in epoch    0      batch 5\n",
      "current in epoch    0      batch 6\n",
      "current in epoch    0      batch 7\n",
      "current in epoch    0      batch 8\n",
      "current in epoch    0      batch 9\n",
      "current in epoch    0      batch 10\n",
      "current in epoch    0      batch 11\n",
      "current in epoch    0      batch 12\n",
      "current in epoch    0      batch 13\n",
      "current in epoch    0      batch 14\n",
      "current in epoch    0      batch 15\n",
      "current in epoch    0      batch 16\n",
      "current in epoch    0      batch 17\n",
      "current in epoch    0      batch 18\n",
      "current in epoch    0      batch 19\n",
      "current in epoch    0      batch 20\n",
      "current in epoch    0      batch 21\n",
      "current in epoch    0      batch 22\n",
      "current in epoch    0      batch 23\n",
      "current in epoch    0      batch 24\n",
      "current in epoch    0      batch 25\n",
      "current in epoch    0      batch 26\n",
      "current in epoch    0      batch 27\n",
      "========================================\n",
      "Epoch 1/1000 - partial_train_loss: 12319.3851 \n",
      "sorting training set\n",
      "Epoch 1/1000 - Training loss: 567.0594 \n",
      "========================================\n",
      "Epoch: [1/1000], TrainLoss: 597.0492966147533\n",
      "current in epoch    1      batch 0\n",
      "current in epoch    1      batch 1\n",
      "current in epoch    1      batch 2\n",
      "current in epoch    1      batch 3\n",
      "current in epoch    1      batch 4\n",
      "current in epoch    1      batch 5\n",
      "current in epoch    1      batch 6\n",
      "current in epoch    1      batch 7\n",
      "current in epoch    1      batch 8\n",
      "current in epoch    1      batch 9\n",
      "current in epoch    1      batch 10\n",
      "current in epoch    1      batch 11\n",
      "current in epoch    1      batch 12\n",
      "current in epoch    1      batch 13\n",
      "current in epoch    1      batch 14\n",
      "current in epoch    1      batch 15\n",
      "current in epoch    1      batch 16\n",
      "current in epoch    1      batch 17\n",
      "current in epoch    1      batch 18\n",
      "current in epoch    1      batch 19\n",
      "current in epoch    1      batch 20\n",
      "current in epoch    1      batch 21\n",
      "current in epoch    1      batch 22\n",
      "current in epoch    1      batch 23\n",
      "current in epoch    1      batch 24\n",
      "current in epoch    1      batch 25\n",
      "current in epoch    1      batch 26\n",
      "current in epoch    1      batch 27\n",
      "========================================\n",
      "Epoch 2/1000 - partial_train_loss: 145.7355 \n",
      "Epoch: [2/1000], TrainLoss: 2.0542189947196414\n",
      "current in epoch    2      batch 0\n",
      "current in epoch    2      batch 1\n",
      "current in epoch    2      batch 2\n",
      "current in epoch    2      batch 3\n",
      "current in epoch    2      batch 4\n",
      "current in epoch    2      batch 5\n",
      "current in epoch    2      batch 6\n",
      "current in epoch    2      batch 7\n",
      "current in epoch    2      batch 8\n",
      "current in epoch    2      batch 9\n",
      "current in epoch    2      batch 10\n",
      "current in epoch    2      batch 11\n",
      "current in epoch    2      batch 12\n",
      "current in epoch    2      batch 13\n",
      "current in epoch    2      batch 14\n",
      "current in epoch    2      batch 15\n",
      "current in epoch    2      batch 16\n",
      "current in epoch    2      batch 17\n",
      "current in epoch    2      batch 18\n",
      "current in epoch    2      batch 19\n",
      "current in epoch    2      batch 20\n",
      "current in epoch    2      batch 21\n",
      "current in epoch    2      batch 22\n",
      "current in epoch    2      batch 23\n",
      "current in epoch    2      batch 24\n",
      "current in epoch    2      batch 25\n",
      "current in epoch    2      batch 26\n",
      "current in epoch    2      batch 27\n",
      "========================================\n",
      "Epoch 3/1000 - partial_train_loss: 3.8927 \n",
      "Epoch: [3/1000], TrainLoss: 2.2538006944315776\n",
      "training Loss has not improved for 1 epochs.\n",
      "current in epoch    3      batch 0\n",
      "current in epoch    3      batch 1\n",
      "current in epoch    3      batch 2\n",
      "current in epoch    3      batch 3\n",
      "current in epoch    3      batch 4\n",
      "current in epoch    3      batch 5\n",
      "current in epoch    3      batch 6\n",
      "current in epoch    3      batch 7\n",
      "current in epoch    3      batch 8\n",
      "current in epoch    3      batch 9\n",
      "current in epoch    3      batch 10\n",
      "current in epoch    3      batch 11\n",
      "current in epoch    3      batch 12\n",
      "current in epoch    3      batch 13\n",
      "current in epoch    3      batch 14\n",
      "current in epoch    3      batch 15\n",
      "current in epoch    3      batch 16\n",
      "current in epoch    3      batch 17\n",
      "current in epoch    3      batch 18\n",
      "current in epoch    3      batch 19\n",
      "current in epoch    3      batch 20\n",
      "current in epoch    3      batch 21\n",
      "current in epoch    3      batch 22\n",
      "current in epoch    3      batch 23\n",
      "current in epoch    3      batch 24\n",
      "current in epoch    3      batch 25\n",
      "current in epoch    3      batch 26\n",
      "current in epoch    3      batch 27\n",
      "========================================\n",
      "Epoch 4/1000 - partial_train_loss: 1.6404 \n",
      "Epoch: [4/1000], TrainLoss: 0.8502245034490313\n",
      "current in epoch    4      batch 0\n",
      "grad: fc128.weight          torch.Size([128, 128])\n",
      "grad: fc128.bias          torch.Size([128])\n",
      "grad: fc64.weight          torch.Size([64, 64])\n",
      "grad: fc64.bias          torch.Size([64])\n",
      "grad: fc16.weight          torch.Size([16, 16])\n",
      "grad: fc16.bias          torch.Size([16])\n",
      "grad: bn128.weight          torch.Size([128])\n",
      "grad: bn128.bias          torch.Size([128])\n",
      "grad: bn64.weight          torch.Size([64])\n",
      "grad: bn64.bias          torch.Size([64])\n",
      "grad: bn16.weight          torch.Size([16])\n",
      "grad: bn16.bias          torch.Size([16])\n",
      "grad: inputlayer.weight          torch.Size([128, 150])\n",
      "grad: inputlayer.bias          torch.Size([128])\n",
      "grad: con128_64.weight          torch.Size([64, 128])\n",
      "grad: con128_64.bias          torch.Size([64])\n",
      "grad: con64_16.weight          torch.Size([16, 64])\n",
      "grad: con64_16.bias          torch.Size([16])\n",
      "grad: output16.weight          torch.Size([1, 16])\n",
      "grad: output16.bias          torch.Size([1])\n",
      "dloss_dtheta.shape:  256\n",
      "RLoss: 6099.64892578125\n",
      "current in epoch    4      batch 1\n",
      "RLoss: 5933.64599609375\n",
      "current in epoch    4      batch 2\n",
      "RLoss: 4875.1923828125\n",
      "current in epoch    4      batch 3\n",
      "RLoss: 400.58587646484375\n",
      "current in epoch    4      batch 4\n",
      "RLoss: 4613.70166015625\n",
      "current in epoch    4      batch 5\n",
      "RLoss: 697.9755249023438\n",
      "current in epoch    4      batch 6\n",
      "RLoss: 140.5419464111328\n",
      "current in epoch    4      batch 7\n",
      "RLoss: 186.06764221191406\n",
      "current in epoch    4      batch 8\n",
      "RLoss: 265.1065368652344\n",
      "current in epoch    4      batch 9\n",
      "RLoss: 93.6253890991211\n",
      "current in epoch    4      batch 10\n",
      "RLoss: 45.62251281738281\n",
      "current in epoch    4      batch 11\n",
      "RLoss: 64.18216705322266\n",
      "current in epoch    4      batch 12\n",
      "RLoss: 426.9063415527344\n",
      "current in epoch    4      batch 13\n",
      "RLoss: 454.061279296875\n",
      "current in epoch    4      batch 14\n",
      "RLoss: 212.40164184570312\n",
      "current in epoch    4      batch 15\n",
      "RLoss: 84.25359344482422\n",
      "current in epoch    4      batch 16\n",
      "RLoss: 396.2779541015625\n",
      "current in epoch    4      batch 17\n",
      "RLoss: 136.8028564453125\n",
      "current in epoch    4      batch 18\n",
      "RLoss: 190.58547973632812\n",
      "current in epoch    4      batch 19\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "min_weight = torch.min(method.weights)\n",
    "attack_gamma = (1.0 / min_weight).data\n",
    "criterion = nn.MSELoss()\n",
    "epoch = 0\n",
    "zero_list = []\n",
    "end_flag = False\n",
    "\n",
    "method.model =IRNet_intorch(config_dict['dim_x']).to(device)\n",
    "method.model.optimizer = optim.Adam(method.model.parameters(), lr=config_dict['network_lr'])\n",
    "\n",
    "while epoch <=config_dict['num_epochs']:\n",
    "    train_loss = 0.0\n",
    "    Rsplt1_test_mse_loss = 0\n",
    "    Rsplt2_test_mse_loss = 0\n",
    "    Rsplt3_test_mse_loss = 0\n",
    "    Rsplt4_test_mse_loss = 0\n",
    "    Rsplt5_test_mse_loss = 0\n",
    "    Rsplt_test_mse_loss = 0\n",
    "    Xshft_test_mse_loss = 0\n",
    "    pizeo_test_mse_loss = 0\n",
    "    statY_test_mse_loss = 0\n",
    "    infoY_test_mse_loss = 0\n",
    "    partial_train_loss = 0.0\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    minima = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(f\"current in epoch    {epoch}      batch {batch_idx}\")\n",
    "        \n",
    "         # Zero the gradients\n",
    "        method.model.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs.to(device) , target.float().to(device))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        method.model.optimizer.step()\n",
    "        partial_train_loss += loss.cpu().item() \n",
    "        if epoch <config_dict['FT_epoch']:\n",
    "            continue\n",
    "        \n",
    "        method.train_theta((data, target), config_dict['theta_epoch'], config_dict['attack_epoch'], attack_gamma, end_flag)\n",
    "        rtheta = method.r([[data, target]], alpha=config_dict['alpha'] / math.sqrt(epoch + 1))\n",
    "        method.theta_grad = grad(rtheta, list(method.model.parameters()), create_graph=True, allow_unused=True)\n",
    "        dr_dx = torch.matmul(method.theta_grad[config_dict['grad_layer']].reshape(-1), method.xa_grad).squeeze()\n",
    "        deltaw = dr_dx * method.weight_grad\n",
    "        deltaw = torch.sum(deltaw, 0)\n",
    "        \n",
    "        deltaw[zero_list] = 0.0\n",
    "        max_grad = torch.max(torch.abs(deltaw))\n",
    "        deltastep = config_dict['deltaall']\n",
    "        lr_weight = (deltastep / max_grad).detach()\n",
    "        print(f'RLoss: {rtheta.data}')\n",
    "\n",
    "\n",
    "        '''\n",
    "        if epoch %50==0:\n",
    "            #save adv ori advadv data\n",
    "            images, labels = method.adversarial_data \n",
    "            adv_data_list = data.numpy()\n",
    "            #adv_labels_list = labels.tolist()\n",
    "            \n",
    "            images, labels = method.adv_based_on \n",
    "            ori_data_list = data.numpy()\n",
    "            ori_labels_list = labels.numpy()\n",
    "            \n",
    "            #images, labels = method.adv_again \n",
    "            #advadv_data_list = data.numpy()\n",
    "            #advadv_labels_list = labels.tolist()\n",
    "            \n",
    "            \n",
    "            csv_filename = f'adv_data_labels_{epoch}_{batch_idx}.csv' if method.adv_again is None else f'adv_adv_data_labels_{epoch}_{batch_idx}.csv'\n",
    "            \n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "            \n",
    "                # Write header\n",
    "                header = [f'feature_{i}' for i in range(ori_data_list.shape[1])] + ['label']\n",
    "                writer.writerow(header)\n",
    "            \n",
    "                # Write data and labels for each group\n",
    "                rows_group1 = np.column_stack((ori_data_list, ori_labels_list))\n",
    "                rows_group2 = np.column_stack((adv_data_list, ori_labels_list))\n",
    "            \n",
    "                # Check if data_group3 is not None before including it\n",
    "                if method.adv_again is not None:\n",
    "                    images, labels = method.adv_again \n",
    "                    advadv_data_list = data.numpy()\n",
    "                    rows_group3 = np.column_stack((advadv_data_list, ori_labels_list))\n",
    "            \n",
    "                    # Combine data from all groups\n",
    "                    all_rows = np.vstack((rows_group1, rows_group2, rows_group3))\n",
    "                else:\n",
    "                    # Combine data from the first two groups only\n",
    "                    all_rows = np.vstack((rows_group1, rows_group2))\n",
    "            \n",
    "                writer.writerows(all_rows)\n",
    "        '''\n",
    "        if epoch >config_dict['partial_epoch_start']:\n",
    "\n",
    "            if batch_idx ==config_dict['partial_batch']:# train partial trainset\n",
    "                break\n",
    "        \n",
    "    \n",
    "\n",
    "    partial_train_loss /= (batch_idx+1)        \n",
    "    print(\"==\"*20)\n",
    "    print(f\"Epoch {epoch+1}/{config_dict['num_epochs']} - partial_train_loss: {partial_train_loss:.4f} \")\n",
    "    if epoch %config_dict['partial_epoch_every']==0:\n",
    "        with torch.no_grad():\n",
    "                print(\"sorting training set\")\n",
    "                # for sorting Training set\n",
    "                sort_MAE=pd.DataFrame(columns = ['data', 'label', 'loss'])\n",
    "                method.model.eval()\n",
    "                for i in range(len(train_dataset.inputs)):\n",
    "                    inp = train_dataset.inputs[i]\n",
    "                    tar = train_dataset.labels[i]\n",
    "\n",
    "                    x = torch.tensor([inp.tolist()], dtype=torch.float32).to(device) \n",
    "                    y = torch.tensor(tar.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "                    output = method.model(x)\n",
    "                    loss = criterion(output, y).cpu()\n",
    "                    # Accumulate the training loss\n",
    "                    train_loss += loss.item() \n",
    "                    #print(f\"loss:       {loss}\")\n",
    "                    sort_MAE = sort_MAE.append({'data' : inp, \n",
    "                                                'label' :tar, \n",
    "                                                'loss' : loss},\n",
    "                                                ignore_index = True)\n",
    "\n",
    "\n",
    "                if epoch%config_dict['partial_epoch_save']==0:\n",
    "                    sort_MAE.to_csv(f\"{config_dict['exp-dir']}sorted/train_set_sorting_{epoch}.csv\",index=False)\n",
    "                new_train = sort_MAE.sort_values(by=['loss'],ascending=False)\n",
    "                new_train_dataset = RecurrentDataset(new_train)\n",
    "                train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "\n",
    "                train_loss /= len(train_dataset.inputs)\n",
    "                print(f\"Epoch {epoch+1}/{config_dict['num_epochs']} - Training loss: {train_loss:.4f} \")\n",
    "                print(\"==\"*20)\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            train_loss += loss.item() \n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    print(f'Epoch: [{(epoch + 1)}/{ config_dict[\"num_epochs\"] }], TrainLoss: {train_loss}')\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset1_loader):\n",
    "        \n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt1_test_mse_loss += loss.item() \n",
    "    Rsplt1_test_mse_loss /= len(Rsplt_testset1_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset2_loader):\n",
    "        \n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt2_test_mse_loss += loss.item() \n",
    "    Rsplt2_test_mse_loss /= len(Rsplt_testset2_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset3_loader):\n",
    "        \n",
    "            outputs = method.model(data.to(device))\n",
    "            loss = criterion(outputs , target.float().to(device))\n",
    "            Rsplt3_test_mse_loss += loss.item() \n",
    "    Rsplt3_test_mse_loss /= len(Rsplt_testset3_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset4_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt4_test_mse_loss += loss.item() \n",
    "    Rsplt4_test_mse_loss /= len(Rsplt_testset4_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_testset5_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt5_test_mse_loss += loss.item() \n",
    "    Rsplt5_test_mse_loss /= len(Rsplt_testset5_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Rsplt_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Rsplt_test_mse_loss += loss.item() \n",
    "    Rsplt_test_mse_loss /= len(Rsplt_test_loader)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(Xshft_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        Xshft_test_mse_loss += loss.item() \n",
    "    Xshft_test_mse_loss /= len(Xshft_test_loader)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(pizeo_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        pizeo_test_mse_loss += loss.item() \n",
    "    pizeo_test_mse_loss /= len(pizeo_test_loader)\n",
    "            \n",
    "    for batch_idx, (data, target) in enumerate(statY_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        statY_test_mse_loss += loss.item() \n",
    "    statY_test_mse_loss /= len(statY_test_loader)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(infoY_test_loader):\n",
    "    \n",
    "        outputs = method.model(data.to(device))\n",
    "        loss = criterion(outputs , target.float().to(device))\n",
    "        infoY_test_mse_loss += loss.item() \n",
    "    infoY_test_mse_loss /= len(infoY_test_loader)\n",
    "\n",
    "\n",
    "    rsplt_ave = np.average([\n",
    "                                Rsplt1_test_mse_loss,\n",
    "                                Rsplt2_test_mse_loss,\n",
    "                                Rsplt3_test_mse_loss,\n",
    "                                Rsplt4_test_mse_loss,\n",
    "                                Rsplt5_test_mse_loss\n",
    "                                ])\n",
    "\n",
    "    save =[]\n",
    "    if partial_train_loss < partial_train_best_loss:\n",
    "        partial_train_best_loss = partial_train_loss\n",
    "        save.append(\"partialTrain\")\n",
    "    if Rsplt1_test_mse_loss < Rsplt_testset1_best_loss:\n",
    "            Rsplt_testset1_best_loss  =  Rsplt1_test_mse_loss\n",
    "            save.append(\"Rsplt1\")\n",
    "    if Rsplt2_test_mse_loss < Rsplt_testset2_best_loss:\n",
    "            Rsplt_testset2_best_loss  =  Rsplt2_test_mse_loss\n",
    "            save.append(\"Rsplt2\")\n",
    "    if Rsplt3_test_mse_loss < Rsplt_testset3_best_loss:\n",
    "            Rsplt_testset3_best_loss  =  Rsplt3_test_mse_loss\n",
    "            save.append(\"Rsplt3\")\n",
    "    if Rsplt4_test_mse_loss < Rsplt_testset4_best_loss:\n",
    "            Rsplt_testset4_best_loss  =  Rsplt4_test_mse_loss\n",
    "            save.append(\"Rsplt4\")\n",
    "    if Rsplt5_test_mse_loss < Rsplt_testset5_best_loss:\n",
    "            Rsplt_testset5_best_loss  =  Rsplt5_test_mse_loss\n",
    "            save.append(\"Rsplt5\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if Xshft_test_mse_loss < Xshft_test_best_loss:\n",
    "            Xshft_test_best_loss  =  Xshft_test_mse_loss\n",
    "            save.append(\"Xshft\")\n",
    "    if pizeo_test_mse_loss < pizeo_test_best_loss:\n",
    "            pizeo_test_best_loss  =  pizeo_test_mse_loss\n",
    "            save.append(\"pizeo\")\n",
    "    if statY_test_mse_loss < statY_test_best_loss:\n",
    "            statY_test_best_loss  =  statY_test_mse_loss\n",
    "            save.append(\"statY\")\n",
    "    if infoY_test_mse_loss < infoY_test_best_loss:\n",
    "            infoY_test_best_loss  =  infoY_test_mse_loss\n",
    "            save.append(\"infoY\")\n",
    "\n",
    "    \n",
    "    # Stop the training process if the training loss has stopped decreasing or has started to increase\n",
    "    if train_loss < train_best_loss:\n",
    "        train_best_loss = train_loss\n",
    "        counter = 0\n",
    "        torch.save(method.model, f'{config_dict[\"exp-dir\"]}models/IR3_epoch_{epoch}.pt')\n",
    "        torch.save(method.weights, f\"{config_dict['exp-dir']}models/SAL_weight_{epoch}_gamma_{attack_gamma}.pt\")\n",
    "        save.append(\"Train\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "        print(f'training Loss has not improved for {counter} epochs.')\n",
    "            \n",
    "   \n",
    "   \n",
    "    if rsplt_ave < Rsplt_test_best_loss:\n",
    "            Rsplt_test_best_loss  =  rsplt_ave\n",
    "            counter_val = 0\n",
    "            torch.save(method.model,f\"{config_dict['exp-dir']}models/IR3_SAL-bset-Rsplt_test_mse_loss.pt\")\n",
    "            save.append(\"Rsplt_AVE\")\n",
    "    else:\n",
    "        counter_val += 1\n",
    "        if counter_val >= config_dict['early_stop']:\n",
    "            print(f'Training stopped. Valid (rand) Loss has not improved for {500} epochs.')\n",
    "            break\n",
    "\n",
    "\n",
    "    entry = [epoch, \n",
    "                                    f\"{train_loss:.4f}\",\n",
    "                                    f\"{partial_train_loss:.4f}\",\n",
    "                                    f\"{Rsplt1_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt2_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt3_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt4_test_mse_loss:.4f}\",\n",
    "                                    f\"{Rsplt5_test_mse_loss:.4f}\",\n",
    "                                    f\"{rsplt_ave:.4f}\",\n",
    "\n",
    "                                    f\"{Xshft_test_mse_loss:.4f}\",\n",
    "                                    f\"{pizeo_test_mse_loss:.4f}\",\n",
    "                                    f\"{statY_test_mse_loss:.4f}\",\n",
    "                                    f\"{infoY_test_mse_loss:.4f}\",\n",
    "\n",
    "                                    f\"                         \", \n",
    "\n",
    "                                    f\"{train_best_loss:.4f}\", \n",
    "                                    f\"{partial_train_best_loss:.4f}\",\n",
    "                                    f\"{Rsplt_testset1_best_loss:.4f}\", \n",
    "                                    f\"{Rsplt_testset2_best_loss:.4f}\", \n",
    "                                    f\"{Rsplt_testset3_best_loss:.4f}\", \n",
    "                                    f\"{Rsplt_testset4_best_loss:.4f}\", \n",
    "                                    f\"{Rsplt_testset5_best_loss:.4f}\", \n",
    "\n",
    "                                    f\"{Rsplt_test_best_loss:.4f}\", \n",
    "                                    f\"{Xshft_test_best_loss:.4f}\", \n",
    "                                    f\"{pizeo_test_best_loss:.4f}\", \n",
    "                                    f\"{statY_test_best_loss:.4f}\", \n",
    "                                    f\"{infoY_test_best_loss:.4f}\", \n",
    "\n",
    "                                    save    ,\n",
    "                                    f'{attack_gamma}'\n",
    "                                    ]\n",
    "    loss_df.loc[len(loss_df)] = entry\n",
    "\n",
    "    loss_df.to_csv(config_dict['exp-dir']+'SAL-training_loss.csv', index=False)\n",
    "    epoch=epoch+1\n",
    "\n",
    "    # adjust gamma according to min(weight)\n",
    "\n",
    "    for i in range(method.weights.shape[0]):\n",
    "        if method.weights[i] > 0.0 and method.weights[i] < min_weight:\n",
    "            min_weight = method.weights[i]\n",
    "        if method.weights[i] < 0.0:\n",
    "            method.weights[i] = 1.0\n",
    "            zero_list.append(i)\n",
    "\n",
    "    attack_gamma = (1.0 / min_weight).data\n",
    "    if epoch <=config_dict['FT_epoch']:\n",
    "        continue\n",
    "\n",
    "    method.weights -= lr_weight * deltaw.detach().reshape(method.weights.shape)\n",
    "    del rtheta\n",
    "    del dr_dx\n",
    "    del deltaw\n",
    "    del max_grad\n",
    "    del deltastep\n",
    "    del lr_weight\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(method.weights, f\"Whole_SAL_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(method.model, f\"IR3_epoch_{epoch}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('OOD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9527c1aa2febaf9c7bab479ad701127eaf7cbb2c19ab2b26ad381c63904c9a00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
